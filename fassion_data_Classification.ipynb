{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport os\nimport math\nimport cv2","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pytorch utility imports\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision.utils import make_grid\n\n#neural net imports\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/fashionmnist/fashion-mnist_train.csv')\ntest_df = pd.read_csv('../input/fashionmnist/fashion-mnist_test.csv')\nprint(train_df.head(2))\ntest_df.head(2)","execution_count":3,"outputs":[{"output_type":"stream","text":"   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n0      2       0       0       0       0       0       0       0       0   \n1      9       0       0       0       0       0       0       0       0   \n\n   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n\n   pixel781  pixel782  pixel783  pixel784  \n0         0         0         0         0  \n1         0         0         0         0  \n\n[2 rows x 785 columns]\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n0      0       0       0       0       0       0       0       0       9   \n1      1       0       0       0       0       0       0       0       0   \n\n   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n0       8  ...       103        87        56         0         0         0   \n1       0  ...        34         0         0         0         0         0   \n\n   pixel781  pixel782  pixel783  pixel784  \n0         0         0         0         0  \n1         0         0         0         0  \n\n[2 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>...</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n      <th>pixel784</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>8</td>\n      <td>...</td>\n      <td>103</td>\n      <td>87</td>\n      <td>56</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels = data_df['label'].values.tolist()\ntrain_labels = train_df['label'].to_numpy()\ntest_labels = test_df['label'].to_numpy()\n# print(labels)\nprint(train_labels[:3])\nprint(train_labels.shape)\nprint(test_labels[:3])\nprint(test_labels.shape)","execution_count":4,"outputs":[{"output_type":"stream","text":"[2 9 6]\n(60000,)\n[0 1 2]\n(10000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# img_data = data_df.iloc[:,1:].values.tolist()\ntrain_img_data = train_df.iloc[:,1:].to_numpy()\ntest_img_data = test_df.iloc[:,1:].to_numpy()\nprint(train_img_data.shape)\nprint(test_img_data.shape)\n\ntrain_img_data = train_img_data.reshape(len(train_img_data),28,28)\ntest_img_data = test_img_data.reshape(len(test_img_data),28,28)\n\nprint(train_img_data.shape)\nprint(test_img_data.shape)","execution_count":5,"outputs":[{"output_type":"stream","text":"(60000, 784)\n(10000, 784)\n(60000, 28, 28)\n(10000, 28, 28)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"label =\"\"\"0 T-shirt/top\n1 Trouser\n2 Pullover\n3 Dress\n4 Coat\n5 Sandal\n6 Shirt\n7 Sneaker\n8 Bag\n9 Ankle boot\"\"\".split(\"\\n\")\nid_2_label ={key:value.split()[-1] for key,value in enumerate(label)} \nlabel_2_id ={value.split()[-1]:key  for key,value in enumerate(label)}\nprint(id_2_label)","execution_count":6,"outputs":[{"output_type":"stream","text":"{0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'boot'}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_image(img,title='Image'):\n    \n    channel = len(img.shape)\n    \n    if channel <= 2:\n        plt.imshow(img, cmap='gray')\n        plt.title(title)\n        plt.axis('off')\n    else:\n        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        plt.imshow(img)\n        plt.title(title)\n        plt.axis('off')","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_image(train_img_data[0])","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CreateModel(nn.Module):\n    \n    def __init__(self):\n        super(CreateModel,self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=3)\n        self.conv2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=3)\n        self.conv3 = nn.Conv2d(in_channels=16,out_channels=64,kernel_size=3)\n        \n        # third hidden layer (Fully connected layer)\n        self.fc1 = nn.Linear(in_features=64*(4 * 4),out_features=120)\n        # fourth hidden layer (Fully connected layer)\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        #output layer\n        self.out= nn.Linear(in_features=84,out_features=10)\n    \n   \n    def forward(self, t): #flow of the data/tensor\n#         input layer\n#         print('input : ',t.size())\n        t=t.float()\n#         hidden layer 1\n        t=self.conv1(t)\n#         print('conv1 : ',t.size())\n        t=F.relu(t)   #   convolution operation\n        t=F.max_pool2d(t,kernel_size =2, stride=2)\n#         print('max_poll1 : ',t.size())\n#         hidden layer 2\n        t=self.conv2(t)\n#         print('conv2 : ',t.size())\n        t=F.relu(t)\n        t=F.max_pool2d(t,kernel_size=2,stride=1)\n#         print('max_poll2 : ',t.size())\n#         hidden layer 3\n        t=self.conv3(t)\n#         print('conv3 : ',t.size())\n        t=F.relu(t)\n        t=F.max_pool2d(t,kernel_size=2,stride=2)\n#         print('max_poll2 : ',t.size())\n#         flatten\n#         t=t.reshape(-1, 64*(4*4))\n        t = t.view(-1, self.num_flat_features(t))\n#         print('before fc : ',t.size())\n#         t=t.view(-1, (64*4*4))\n#         hidden layer 4 (fully connected)\n        t=self.fc1(t)\n#         print('fc1 : ',t.size())\n        t=F.relu(t)\n#         hidden layer 5 (fully connected)\n        t = self.fc2(t)\n#         print('fc2 : ',t.size())\n        t = F.relu(t)\n#         output layer\n        t=self.out(t)\n#         print(t)\n        t=F.log_softmax(t,1)\n        return t\n    \n    def num_flat_features(self, x):\n#         print('num_flat : ', x.size())\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n#         print('num_features in : ',num_features)\n        return num_features\n    \n    ","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_val, x_test, y_val, y_test = train_test_split(test_img_data, test_labels, test_size=0.5, random_state=42)\n\nprint(x_val.shape,' ', x_test.shape)","execution_count":22,"outputs":[{"output_type":"stream","text":"(5000, 28, 28)   (5000, 28, 28)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = torch.from_numpy(train_img_data)\ny_train =torch.from_numpy(train_labels)\n\n\n# x_test_data = torch.from_numpy(x_test)\n# y_test_data= torch.from_numpy(y_test)\n\nx_val = torch.from_numpy(x_val)\ny_val= torch.from_numpy(y_val)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(num_epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_dataloader):\n        data, target = data, target\n        data = data.reshape(batch_size,-1,28,28)\n#         print('data.size :',data.size())\n#         data = data.unsqueeze(1)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n            \n        optimizer.zero_grad()\n        output = model(data)\n        loss = compute_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        if (batch_idx + 1)% 60 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                num_epoch, (batch_idx + 1) * len(data), len(train_dataloader.dataset),\n                100. * (batch_idx + 1) / len(train_dataloader), loss.item()))\n#         break\n            \ndef evaluate(val_dataloader):\n    model.eval()\n    loss = 0\n    correct = 0\n    for data, target in val_dataloader:\n#         data = data.unsqueeze(1)\n        data, target = data, target\n        data=data.reshape(batch_size,-1,28,28)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        \n        output = model(data)\n        \n        loss +=compute_loss(output, target).item()\n\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n        \n    loss /= len(test_dataloader.dataset)\n        \n    print('\\nAverage Val Loss: {:.4f}, Val Accuracy: {}/{} ({:.3f}%)\\n'.format(\n        loss, correct, len(val_dataloader.dataset),\n        100. * correct / len(val_dataloader.dataset)))\n\ndef predict(model,img=[],desc=False,topk=5,class_name_dict=None):\n    correct=0\n    size = len(img)\n#     print('betch size :',size)\n    w,h=img[0].shape\n    if size == 1:\n        img=torch.from_numpy(img[0])\n    else:\n        img=torch.from_numpy(img)\n    img= img.reshape(size,-1,w,h)\n\n    if torch.cuda.is_available():\n        model = model.cuda()\n        img= img.cuda()\n    output = model(img)\n    if desc:\n        infer_pred(class_name_dict,output,betch=size)\n    pred = output.cpu().data.max(1, keepdim=True)[1]\n    pred = pred.numpy()\n    if size==1:\n        print('Predicted id : ', pred[0])\n    return pred\n\n        \ndef infer_pred(id_2_label,output,topk=5, betch=1):\n    pred_prob= nn.functional.softmax(output)\n    _,top_K_prob=pred_prob.topk(topk)\n    top_K_prob =top_K_prob.cpu().numpy()\n    pred_prob=pred_prob.cpu().detach().numpy()\n    c=0\n    for j in range(betch):\n        print('\\ntop {} are confidance scores are for Betch ({})  : '.format(topk, j))\n        print('No   Pred_id   label      confidance')\n        for i in range(topk):\n            c=c+1\n            print(\"{},    {},       {},     {} %\\\n            \".format(c,top_K_prob[j][i],id_2_label[top_K_prob[j][i]],pred_prob[j][i]*100))\n#         print(top_K_prob[j])\n#         max_id=np.where(top_K_prob[j] == np.amax(top_K_prob[j]))[0][0]\n        print('\\nPredicted Class: **{}**'.format(id_2_label[np.amax(top_K_prob[j])]))\n    \ndef get_correct_prediction(id_2_label,pred, labels):\n    infer_list=[]\n    match_list=[]\n    incorrect_list=[]\n    correct =0\n    infer_list.append(['index' ,'Classs Name','Predicted', 'True value', 'Mismatch'])\n    for idx,y in enumerate(pred):\n        if y==labels[idx]:\n            match_list.append(idx)\n            correct +=1\n            infer_list.append([idx,id_2_label[y[0]],y[0],labels[idx],True])\n        else:\n            incorrect_list.append((idx, y[0],labels[idx]))\n            infer_list.append([idx,id_2_label[y[0]],y[0],labels[idx],False])\n    print('Totla : {} , correct prediciton : {} , incorrect : {} \\n'.format(len(labels),correct,len(labels)- correct))\n    print('Accuracy : {}%'.format(100. * correct / len(labels)))\n    return tuple(infer_list), tuple(match_list), tuple(incorrect_list)\n    ","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CreateModel()\n\nif torch.cuda.is_available():\n    model = model.cuda()\nbatch_size=1000\n# batch_size=1\nepochs = 200\ncompute_loss = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n# optimizer = optim.Adam(params=conv_model.parameters(), lr=0.003)\n","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_dataset=TensorDataset(x_train,y_train) # create your datset\ntrain_dataloader = DataLoader(train_dataset,batch_size=batch_size) # create your dataloader\n\n# test_dataset=TensorDataset(x_test,y_test) # create your datset\n# test_dataloader =DataLoader(train_dataset,batch_size=batch_size) # create your dataloader\n\nval_dataset=TensorDataset(x_val,y_val) # create your datset\nval_dataloader =DataLoader(val_dataset,batch_size=batch_size) # create your dataloader\n\nprint(type(train_dataset))\nprint(type(train_dataloader))","execution_count":63,"outputs":[{"output_type":"stream","text":"<class 'torch.utils.data.dataset.TensorDataset'>\n<class 'torch.utils.data.dataloader.DataLoader'>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor i in range(epochs):\n    print('starting training..\\n')\n    train_model(i)\n    print('starting evalation \\n')\n    evaluate(val_dataloader)\n    ","execution_count":64,"outputs":[{"output_type":"stream","text":"starting training..\n\nTrain Epoch: 0 [60000/60000 (100%)]\tLoss: 0.679518\nstarting evalation \n\n\nAverage Val Loss: 0.0001, Val Accuracy: 3755/5000 (75.100%)\n\nstarting training..\n\nTrain Epoch: 1 [60000/60000 (100%)]\tLoss: 0.581099\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 3977/5000 (79.540%)\n\nstarting training..\n\nTrain Epoch: 2 [60000/60000 (100%)]\tLoss: 0.507257\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4060/5000 (81.200%)\n\nstarting training..\n\nTrain Epoch: 3 [60000/60000 (100%)]\tLoss: 0.471796\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4126/5000 (82.520%)\n\nstarting training..\n\nTrain Epoch: 4 [60000/60000 (100%)]\tLoss: 0.443883\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4209/5000 (84.180%)\n\nstarting training..\n\nTrain Epoch: 5 [60000/60000 (100%)]\tLoss: 0.403345\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4210/5000 (84.200%)\n\nstarting training..\n\nTrain Epoch: 6 [60000/60000 (100%)]\tLoss: 0.411444\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4263/5000 (85.260%)\n\nstarting training..\n\nTrain Epoch: 7 [60000/60000 (100%)]\tLoss: 0.379773\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4282/5000 (85.640%)\n\nstarting training..\n\nTrain Epoch: 8 [60000/60000 (100%)]\tLoss: 0.365237\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4298/5000 (85.960%)\n\nstarting training..\n\nTrain Epoch: 9 [60000/60000 (100%)]\tLoss: 0.357897\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4330/5000 (86.600%)\n\nstarting training..\n\nTrain Epoch: 10 [60000/60000 (100%)]\tLoss: 0.348808\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4343/5000 (86.860%)\n\nstarting training..\n\nTrain Epoch: 11 [60000/60000 (100%)]\tLoss: 0.341010\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4349/5000 (86.980%)\n\nstarting training..\n\nTrain Epoch: 12 [60000/60000 (100%)]\tLoss: 0.332559\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4365/5000 (87.300%)\n\nstarting training..\n\nTrain Epoch: 13 [60000/60000 (100%)]\tLoss: 0.325652\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4384/5000 (87.680%)\n\nstarting training..\n\nTrain Epoch: 14 [60000/60000 (100%)]\tLoss: 0.319265\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4394/5000 (87.880%)\n\nstarting training..\n\nTrain Epoch: 15 [60000/60000 (100%)]\tLoss: 0.313630\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4399/5000 (87.980%)\n\nstarting training..\n\nTrain Epoch: 16 [60000/60000 (100%)]\tLoss: 0.307882\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4403/5000 (88.060%)\n\nstarting training..\n\nTrain Epoch: 17 [60000/60000 (100%)]\tLoss: 0.302623\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4419/5000 (88.380%)\n\nstarting training..\n\nTrain Epoch: 18 [60000/60000 (100%)]\tLoss: 0.298351\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4424/5000 (88.480%)\n\nstarting training..\n\nTrain Epoch: 19 [60000/60000 (100%)]\tLoss: 0.295951\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4433/5000 (88.660%)\n\nstarting training..\n\nTrain Epoch: 20 [60000/60000 (100%)]\tLoss: 0.293595\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4440/5000 (88.800%)\n\nstarting training..\n\nTrain Epoch: 21 [60000/60000 (100%)]\tLoss: 0.291533\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4436/5000 (88.720%)\n\nstarting training..\n\nTrain Epoch: 22 [60000/60000 (100%)]\tLoss: 0.289441\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4416/5000 (88.320%)\n\nstarting training..\n\nTrain Epoch: 23 [60000/60000 (100%)]\tLoss: 0.282014\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4405/5000 (88.100%)\n\nstarting training..\n\nTrain Epoch: 24 [60000/60000 (100%)]\tLoss: 0.279089\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4418/5000 (88.360%)\n\nstarting training..\n\nTrain Epoch: 25 [60000/60000 (100%)]\tLoss: 0.275239\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4426/5000 (88.520%)\n\nstarting training..\n\nTrain Epoch: 26 [60000/60000 (100%)]\tLoss: 0.270518\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4426/5000 (88.520%)\n\nstarting training..\n\nTrain Epoch: 27 [60000/60000 (100%)]\tLoss: 0.266364\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4433/5000 (88.660%)\n\nstarting training..\n\nTrain Epoch: 28 [60000/60000 (100%)]\tLoss: 0.263003\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4430/5000 (88.600%)\n\nstarting training..\n\nTrain Epoch: 29 [60000/60000 (100%)]\tLoss: 0.259961\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4435/5000 (88.700%)\n\nstarting training..\n\nTrain Epoch: 30 [60000/60000 (100%)]\tLoss: 0.255929\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4436/5000 (88.720%)\n\nstarting training..\n\nTrain Epoch: 31 [60000/60000 (100%)]\tLoss: 0.253162\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4432/5000 (88.640%)\n\nstarting training..\n\nTrain Epoch: 32 [60000/60000 (100%)]\tLoss: 0.250453\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4441/5000 (88.820%)\n\nstarting training..\n\nTrain Epoch: 33 [60000/60000 (100%)]\tLoss: 0.247425\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4445/5000 (88.900%)\n\nstarting training..\n\nTrain Epoch: 34 [60000/60000 (100%)]\tLoss: 0.244519\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4443/5000 (88.860%)\n\nstarting training..\n\nTrain Epoch: 35 [60000/60000 (100%)]\tLoss: 0.242757\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4441/5000 (88.820%)\n\nstarting training..\n\nTrain Epoch: 36 [60000/60000 (100%)]\tLoss: 0.239669\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4445/5000 (88.900%)\n\nstarting training..\n\nTrain Epoch: 37 [60000/60000 (100%)]\tLoss: 0.236251\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4453/5000 (89.060%)\n\nstarting training..\n\nTrain Epoch: 38 [60000/60000 (100%)]\tLoss: 0.233305\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4454/5000 (89.080%)\n\nstarting training..\n\nTrain Epoch: 39 [60000/60000 (100%)]\tLoss: 0.230256\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4458/5000 (89.160%)\n\nstarting training..\n\nTrain Epoch: 40 [60000/60000 (100%)]\tLoss: 0.227972\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4458/5000 (89.160%)\n\nstarting training..\n\nTrain Epoch: 41 [60000/60000 (100%)]\tLoss: 0.225146\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4462/5000 (89.240%)\n\nstarting training..\n\nTrain Epoch: 42 [60000/60000 (100%)]\tLoss: 0.222089\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4457/5000 (89.140%)\n\nstarting training..\n\nTrain Epoch: 43 [60000/60000 (100%)]\tLoss: 0.220696\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4464/5000 (89.280%)\n\nstarting training..\n\nTrain Epoch: 44 [60000/60000 (100%)]\tLoss: 0.218285\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4469/5000 (89.380%)\n\nstarting training..\n\nTrain Epoch: 45 [60000/60000 (100%)]\tLoss: 0.215309\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4473/5000 (89.460%)\n\nstarting training..\n\nTrain Epoch: 46 [60000/60000 (100%)]\tLoss: 0.213989\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4476/5000 (89.520%)\n\nstarting training..\n\nTrain Epoch: 47 [60000/60000 (100%)]\tLoss: 0.212114\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4482/5000 (89.640%)\n\nstarting training..\n\nTrain Epoch: 48 [60000/60000 (100%)]\tLoss: 0.210069\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4483/5000 (89.660%)\n\nstarting training..\n\nTrain Epoch: 49 [60000/60000 (100%)]\tLoss: 0.207906\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4491/5000 (89.820%)\n\nstarting training..\n\nTrain Epoch: 50 [60000/60000 (100%)]\tLoss: 0.204839\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4490/5000 (89.800%)\n\nstarting training..\n\nTrain Epoch: 51 [60000/60000 (100%)]\tLoss: 0.203167\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4491/5000 (89.820%)\n\nstarting training..\n\nTrain Epoch: 52 [60000/60000 (100%)]\tLoss: 0.200858\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4493/5000 (89.860%)\n\nstarting training..\n\n","name":"stdout"},{"output_type":"stream","text":"Train Epoch: 53 [60000/60000 (100%)]\tLoss: 0.198881\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4490/5000 (89.800%)\n\nstarting training..\n\nTrain Epoch: 54 [60000/60000 (100%)]\tLoss: 0.196511\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4500/5000 (90.000%)\n\nstarting training..\n\nTrain Epoch: 55 [60000/60000 (100%)]\tLoss: 0.195282\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4498/5000 (89.960%)\n\nstarting training..\n\nTrain Epoch: 56 [60000/60000 (100%)]\tLoss: 0.193462\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4501/5000 (90.020%)\n\nstarting training..\n\nTrain Epoch: 57 [60000/60000 (100%)]\tLoss: 0.191458\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4504/5000 (90.080%)\n\nstarting training..\n\nTrain Epoch: 58 [60000/60000 (100%)]\tLoss: 0.189096\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4505/5000 (90.100%)\n\nstarting training..\n\nTrain Epoch: 59 [60000/60000 (100%)]\tLoss: 0.186935\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4502/5000 (90.040%)\n\nstarting training..\n\nTrain Epoch: 60 [60000/60000 (100%)]\tLoss: 0.185851\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4511/5000 (90.220%)\n\nstarting training..\n\nTrain Epoch: 61 [60000/60000 (100%)]\tLoss: 0.185454\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4513/5000 (90.260%)\n\nstarting training..\n\nTrain Epoch: 62 [60000/60000 (100%)]\tLoss: 0.185160\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4510/5000 (90.200%)\n\nstarting training..\n\nTrain Epoch: 63 [60000/60000 (100%)]\tLoss: 0.184765\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4503/5000 (90.060%)\n\nstarting training..\n\nTrain Epoch: 64 [60000/60000 (100%)]\tLoss: 0.183938\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4503/5000 (90.060%)\n\nstarting training..\n\nTrain Epoch: 65 [60000/60000 (100%)]\tLoss: 0.183314\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4499/5000 (89.980%)\n\nstarting training..\n\nTrain Epoch: 66 [60000/60000 (100%)]\tLoss: 0.182231\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4498/5000 (89.960%)\n\nstarting training..\n\nTrain Epoch: 67 [60000/60000 (100%)]\tLoss: 0.180664\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4497/5000 (89.940%)\n\nstarting training..\n\nTrain Epoch: 68 [60000/60000 (100%)]\tLoss: 0.179734\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4496/5000 (89.920%)\n\nstarting training..\n\nTrain Epoch: 69 [60000/60000 (100%)]\tLoss: 0.179206\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4509/5000 (90.180%)\n\nstarting training..\n\nTrain Epoch: 70 [60000/60000 (100%)]\tLoss: 0.176291\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4504/5000 (90.080%)\n\nstarting training..\n\nTrain Epoch: 71 [60000/60000 (100%)]\tLoss: 0.175358\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4498/5000 (89.960%)\n\nstarting training..\n\nTrain Epoch: 72 [60000/60000 (100%)]\tLoss: 0.173346\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4491/5000 (89.820%)\n\nstarting training..\n\nTrain Epoch: 73 [60000/60000 (100%)]\tLoss: 0.171941\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4495/5000 (89.900%)\n\nstarting training..\n\nTrain Epoch: 74 [60000/60000 (100%)]\tLoss: 0.170918\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4493/5000 (89.860%)\n\nstarting training..\n\nTrain Epoch: 75 [60000/60000 (100%)]\tLoss: 0.169605\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4494/5000 (89.880%)\n\nstarting training..\n\nTrain Epoch: 76 [60000/60000 (100%)]\tLoss: 0.165566\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4493/5000 (89.860%)\n\nstarting training..\n\nTrain Epoch: 77 [60000/60000 (100%)]\tLoss: 0.162674\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4497/5000 (89.940%)\n\nstarting training..\n\nTrain Epoch: 78 [60000/60000 (100%)]\tLoss: 0.160341\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4498/5000 (89.960%)\n\nstarting training..\n\nTrain Epoch: 79 [60000/60000 (100%)]\tLoss: 0.157427\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4502/5000 (90.040%)\n\nstarting training..\n\nTrain Epoch: 80 [60000/60000 (100%)]\tLoss: 0.155891\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4494/5000 (89.880%)\n\nstarting training..\n\nTrain Epoch: 81 [60000/60000 (100%)]\tLoss: 0.154232\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4487/5000 (89.740%)\n\nstarting training..\n\nTrain Epoch: 82 [60000/60000 (100%)]\tLoss: 0.152770\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4481/5000 (89.620%)\n\nstarting training..\n\nTrain Epoch: 83 [60000/60000 (100%)]\tLoss: 0.155959\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4476/5000 (89.520%)\n\nstarting training..\n\nTrain Epoch: 84 [60000/60000 (100%)]\tLoss: 0.169523\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4458/5000 (89.160%)\n\nstarting training..\n\nTrain Epoch: 85 [60000/60000 (100%)]\tLoss: 0.182268\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4440/5000 (88.800%)\n\nstarting training..\n\nTrain Epoch: 86 [60000/60000 (100%)]\tLoss: 0.161970\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4434/5000 (88.680%)\n\nstarting training..\n\nTrain Epoch: 87 [60000/60000 (100%)]\tLoss: 0.152050\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4462/5000 (89.240%)\n\nstarting training..\n\nTrain Epoch: 88 [60000/60000 (100%)]\tLoss: 0.141994\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4484/5000 (89.680%)\n\nstarting training..\n\nTrain Epoch: 89 [60000/60000 (100%)]\tLoss: 0.140627\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4492/5000 (89.840%)\n\nstarting training..\n\nTrain Epoch: 90 [60000/60000 (100%)]\tLoss: 0.144262\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4479/5000 (89.580%)\n\nstarting training..\n\nTrain Epoch: 91 [60000/60000 (100%)]\tLoss: 0.155561\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4441/5000 (88.820%)\n\nstarting training..\n\nTrain Epoch: 92 [60000/60000 (100%)]\tLoss: 0.167990\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4401/5000 (88.020%)\n\nstarting training..\n\nTrain Epoch: 93 [60000/60000 (100%)]\tLoss: 0.157265\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4435/5000 (88.700%)\n\nstarting training..\n\nTrain Epoch: 94 [60000/60000 (100%)]\tLoss: 0.149790\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4450/5000 (89.000%)\n\nstarting training..\n\nTrain Epoch: 95 [60000/60000 (100%)]\tLoss: 0.163762\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4465/5000 (89.300%)\n\nstarting training..\n\nTrain Epoch: 96 [60000/60000 (100%)]\tLoss: 0.159074\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4462/5000 (89.240%)\n\nstarting training..\n\nTrain Epoch: 97 [60000/60000 (100%)]\tLoss: 0.159187\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4457/5000 (89.140%)\n\nstarting training..\n\nTrain Epoch: 98 [60000/60000 (100%)]\tLoss: 0.152299\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4444/5000 (88.880%)\n\nstarting training..\n\nTrain Epoch: 99 [60000/60000 (100%)]\tLoss: 0.147096\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4436/5000 (88.720%)\n\nstarting training..\n\nTrain Epoch: 100 [60000/60000 (100%)]\tLoss: 0.146518\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4428/5000 (88.560%)\n\nstarting training..\n\nTrain Epoch: 101 [60000/60000 (100%)]\tLoss: 0.144824\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4432/5000 (88.640%)\n\nstarting training..\n\nTrain Epoch: 102 [60000/60000 (100%)]\tLoss: 0.144179\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4439/5000 (88.780%)\n\nstarting training..\n\nTrain Epoch: 103 [60000/60000 (100%)]\tLoss: 0.141520\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4429/5000 (88.580%)\n\nstarting training..\n\nTrain Epoch: 104 [60000/60000 (100%)]\tLoss: 0.135549\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4421/5000 (88.420%)\n\nstarting training..\n\nTrain Epoch: 105 [60000/60000 (100%)]\tLoss: 0.128792\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4434/5000 (88.680%)\n\nstarting training..\n\n","name":"stdout"},{"output_type":"stream","text":"Train Epoch: 106 [60000/60000 (100%)]\tLoss: 0.123893\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4437/5000 (88.740%)\n\nstarting training..\n\nTrain Epoch: 107 [60000/60000 (100%)]\tLoss: 0.120531\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4428/5000 (88.560%)\n\nstarting training..\n\nTrain Epoch: 108 [60000/60000 (100%)]\tLoss: 0.117450\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4444/5000 (88.880%)\n\nstarting training..\n\nTrain Epoch: 109 [60000/60000 (100%)]\tLoss: 0.115723\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4449/5000 (88.980%)\n\nstarting training..\n\nTrain Epoch: 110 [60000/60000 (100%)]\tLoss: 0.115074\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4434/5000 (88.680%)\n\nstarting training..\n\nTrain Epoch: 111 [60000/60000 (100%)]\tLoss: 0.112581\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4439/5000 (88.780%)\n\nstarting training..\n\nTrain Epoch: 112 [60000/60000 (100%)]\tLoss: 0.110398\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4443/5000 (88.860%)\n\nstarting training..\n\nTrain Epoch: 113 [60000/60000 (100%)]\tLoss: 0.107774\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4437/5000 (88.740%)\n\nstarting training..\n\nTrain Epoch: 114 [60000/60000 (100%)]\tLoss: 0.108366\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4444/5000 (88.880%)\n\nstarting training..\n\nTrain Epoch: 115 [60000/60000 (100%)]\tLoss: 0.108174\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4445/5000 (88.900%)\n\nstarting training..\n\nTrain Epoch: 116 [60000/60000 (100%)]\tLoss: 0.106155\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4447/5000 (88.940%)\n\nstarting training..\n\nTrain Epoch: 117 [60000/60000 (100%)]\tLoss: 0.103632\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4440/5000 (88.800%)\n\nstarting training..\n\nTrain Epoch: 118 [60000/60000 (100%)]\tLoss: 0.105670\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4435/5000 (88.700%)\n\nstarting training..\n\nTrain Epoch: 119 [60000/60000 (100%)]\tLoss: 0.108377\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4436/5000 (88.720%)\n\nstarting training..\n\nTrain Epoch: 120 [60000/60000 (100%)]\tLoss: 0.109074\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4407/5000 (88.140%)\n\nstarting training..\n\nTrain Epoch: 121 [60000/60000 (100%)]\tLoss: 0.105952\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4394/5000 (87.880%)\n\nstarting training..\n\nTrain Epoch: 122 [60000/60000 (100%)]\tLoss: 0.099041\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4402/5000 (88.040%)\n\nstarting training..\n\nTrain Epoch: 123 [60000/60000 (100%)]\tLoss: 0.099070\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4394/5000 (87.880%)\n\nstarting training..\n\nTrain Epoch: 124 [60000/60000 (100%)]\tLoss: 0.105706\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4417/5000 (88.340%)\n\nstarting training..\n\nTrain Epoch: 125 [60000/60000 (100%)]\tLoss: 0.117346\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4468/5000 (89.360%)\n\nstarting training..\n\nTrain Epoch: 126 [60000/60000 (100%)]\tLoss: 0.109449\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4475/5000 (89.500%)\n\nstarting training..\n\nTrain Epoch: 127 [60000/60000 (100%)]\tLoss: 0.103453\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4473/5000 (89.460%)\n\nstarting training..\n\nTrain Epoch: 128 [60000/60000 (100%)]\tLoss: 0.109217\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4451/5000 (89.020%)\n\nstarting training..\n\nTrain Epoch: 129 [60000/60000 (100%)]\tLoss: 0.101502\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4445/5000 (88.900%)\n\nstarting training..\n\nTrain Epoch: 130 [60000/60000 (100%)]\tLoss: 0.101381\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4459/5000 (89.180%)\n\nstarting training..\n\nTrain Epoch: 131 [60000/60000 (100%)]\tLoss: 0.121980\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4467/5000 (89.340%)\n\nstarting training..\n\nTrain Epoch: 132 [60000/60000 (100%)]\tLoss: 0.144899\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4446/5000 (88.920%)\n\nstarting training..\n\nTrain Epoch: 133 [60000/60000 (100%)]\tLoss: 0.133061\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4471/5000 (89.420%)\n\nstarting training..\n\nTrain Epoch: 134 [60000/60000 (100%)]\tLoss: 0.159273\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4469/5000 (89.380%)\n\nstarting training..\n\nTrain Epoch: 135 [60000/60000 (100%)]\tLoss: 0.134023\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4468/5000 (89.360%)\n\nstarting training..\n\nTrain Epoch: 136 [60000/60000 (100%)]\tLoss: 0.133616\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4453/5000 (89.060%)\n\nstarting training..\n\nTrain Epoch: 137 [60000/60000 (100%)]\tLoss: 0.129818\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4457/5000 (89.140%)\n\nstarting training..\n\nTrain Epoch: 138 [60000/60000 (100%)]\tLoss: 0.119080\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4462/5000 (89.240%)\n\nstarting training..\n\nTrain Epoch: 139 [60000/60000 (100%)]\tLoss: 0.114027\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4458/5000 (89.160%)\n\nstarting training..\n\nTrain Epoch: 140 [60000/60000 (100%)]\tLoss: 0.113883\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4460/5000 (89.200%)\n\nstarting training..\n\nTrain Epoch: 141 [60000/60000 (100%)]\tLoss: 0.107620\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4460/5000 (89.200%)\n\nstarting training..\n\nTrain Epoch: 142 [60000/60000 (100%)]\tLoss: 0.111448\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4440/5000 (88.800%)\n\nstarting training..\n\nTrain Epoch: 143 [60000/60000 (100%)]\tLoss: 0.109932\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4426/5000 (88.520%)\n\nstarting training..\n\nTrain Epoch: 144 [60000/60000 (100%)]\tLoss: 0.103314\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4447/5000 (88.940%)\n\nstarting training..\n\nTrain Epoch: 145 [60000/60000 (100%)]\tLoss: 0.095894\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4465/5000 (89.300%)\n\nstarting training..\n\nTrain Epoch: 146 [60000/60000 (100%)]\tLoss: 0.094222\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4485/5000 (89.700%)\n\nstarting training..\n\nTrain Epoch: 147 [60000/60000 (100%)]\tLoss: 0.099032\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4493/5000 (89.860%)\n\nstarting training..\n\nTrain Epoch: 148 [60000/60000 (100%)]\tLoss: 0.099844\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4493/5000 (89.860%)\n\nstarting training..\n\nTrain Epoch: 149 [60000/60000 (100%)]\tLoss: 0.100843\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4473/5000 (89.460%)\n\nstarting training..\n\nTrain Epoch: 150 [60000/60000 (100%)]\tLoss: 0.100328\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4471/5000 (89.420%)\n\nstarting training..\n\nTrain Epoch: 151 [60000/60000 (100%)]\tLoss: 0.106741\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4457/5000 (89.140%)\n\nstarting training..\n\nTrain Epoch: 152 [60000/60000 (100%)]\tLoss: 0.107519\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4454/5000 (89.080%)\n\nstarting training..\n\nTrain Epoch: 153 [60000/60000 (100%)]\tLoss: 0.111882\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4460/5000 (89.200%)\n\nstarting training..\n\nTrain Epoch: 154 [60000/60000 (100%)]\tLoss: 0.100176\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4468/5000 (89.360%)\n\nstarting training..\n\nTrain Epoch: 155 [60000/60000 (100%)]\tLoss: 0.094301\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4452/5000 (89.040%)\n\nstarting training..\n\nTrain Epoch: 156 [60000/60000 (100%)]\tLoss: 0.094732\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4451/5000 (89.020%)\n\nstarting training..\n\nTrain Epoch: 157 [60000/60000 (100%)]\tLoss: 0.095260\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4442/5000 (88.840%)\n\nstarting training..\n\nTrain Epoch: 158 [60000/60000 (100%)]\tLoss: 0.097733\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4446/5000 (88.920%)\n\nstarting training..\n\n","name":"stdout"},{"output_type":"stream","text":"Train Epoch: 159 [60000/60000 (100%)]\tLoss: 0.102947\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4454/5000 (89.080%)\n\nstarting training..\n\nTrain Epoch: 160 [60000/60000 (100%)]\tLoss: 0.097104\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4443/5000 (88.860%)\n\nstarting training..\n\nTrain Epoch: 161 [60000/60000 (100%)]\tLoss: 0.085672\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4439/5000 (88.780%)\n\nstarting training..\n\nTrain Epoch: 162 [60000/60000 (100%)]\tLoss: 0.091310\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4433/5000 (88.660%)\n\nstarting training..\n\nTrain Epoch: 163 [60000/60000 (100%)]\tLoss: 0.092928\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4445/5000 (88.900%)\n\nstarting training..\n\nTrain Epoch: 164 [60000/60000 (100%)]\tLoss: 0.091451\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4433/5000 (88.660%)\n\nstarting training..\n\nTrain Epoch: 165 [60000/60000 (100%)]\tLoss: 0.086313\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4431/5000 (88.620%)\n\nstarting training..\n\nTrain Epoch: 166 [60000/60000 (100%)]\tLoss: 0.080865\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4421/5000 (88.420%)\n\nstarting training..\n\nTrain Epoch: 167 [60000/60000 (100%)]\tLoss: 0.076610\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4425/5000 (88.500%)\n\nstarting training..\n\nTrain Epoch: 168 [60000/60000 (100%)]\tLoss: 0.074976\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4422/5000 (88.440%)\n\nstarting training..\n\nTrain Epoch: 169 [60000/60000 (100%)]\tLoss: 0.073013\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4441/5000 (88.820%)\n\nstarting training..\n\nTrain Epoch: 170 [60000/60000 (100%)]\tLoss: 0.072219\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4457/5000 (89.140%)\n\nstarting training..\n\nTrain Epoch: 171 [60000/60000 (100%)]\tLoss: 0.068546\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4474/5000 (89.480%)\n\nstarting training..\n\nTrain Epoch: 172 [60000/60000 (100%)]\tLoss: 0.074800\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4477/5000 (89.540%)\n\nstarting training..\n\nTrain Epoch: 173 [60000/60000 (100%)]\tLoss: 0.077223\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4475/5000 (89.500%)\n\nstarting training..\n\nTrain Epoch: 174 [60000/60000 (100%)]\tLoss: 0.084061\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4464/5000 (89.280%)\n\nstarting training..\n\nTrain Epoch: 175 [60000/60000 (100%)]\tLoss: 0.083329\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4450/5000 (89.000%)\n\nstarting training..\n\nTrain Epoch: 176 [60000/60000 (100%)]\tLoss: 0.086444\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4433/5000 (88.660%)\n\nstarting training..\n\nTrain Epoch: 177 [60000/60000 (100%)]\tLoss: 0.117499\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4461/5000 (89.220%)\n\nstarting training..\n\nTrain Epoch: 178 [60000/60000 (100%)]\tLoss: 0.136160\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4453/5000 (89.060%)\n\nstarting training..\n\nTrain Epoch: 179 [60000/60000 (100%)]\tLoss: 0.107087\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4448/5000 (88.960%)\n\nstarting training..\n\nTrain Epoch: 180 [60000/60000 (100%)]\tLoss: 0.113055\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4423/5000 (88.460%)\n\nstarting training..\n\nTrain Epoch: 181 [60000/60000 (100%)]\tLoss: 0.091824\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4452/5000 (89.040%)\n\nstarting training..\n\nTrain Epoch: 182 [60000/60000 (100%)]\tLoss: 0.088467\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4449/5000 (88.980%)\n\nstarting training..\n\nTrain Epoch: 183 [60000/60000 (100%)]\tLoss: 0.092103\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4446/5000 (88.920%)\n\nstarting training..\n\nTrain Epoch: 184 [60000/60000 (100%)]\tLoss: 0.075785\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4456/5000 (89.120%)\n\nstarting training..\n\nTrain Epoch: 185 [60000/60000 (100%)]\tLoss: 0.074562\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4441/5000 (88.820%)\n\nstarting training..\n\nTrain Epoch: 186 [60000/60000 (100%)]\tLoss: 0.124804\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4423/5000 (88.460%)\n\nstarting training..\n\nTrain Epoch: 187 [60000/60000 (100%)]\tLoss: 0.110145\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4410/5000 (88.200%)\n\nstarting training..\n\nTrain Epoch: 188 [60000/60000 (100%)]\tLoss: 0.100863\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4404/5000 (88.080%)\n\nstarting training..\n\nTrain Epoch: 189 [60000/60000 (100%)]\tLoss: 0.084882\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4410/5000 (88.200%)\n\nstarting training..\n\nTrain Epoch: 190 [60000/60000 (100%)]\tLoss: 0.084911\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4411/5000 (88.220%)\n\nstarting training..\n\nTrain Epoch: 191 [60000/60000 (100%)]\tLoss: 0.094335\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4421/5000 (88.420%)\n\nstarting training..\n\nTrain Epoch: 192 [60000/60000 (100%)]\tLoss: 0.107836\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4426/5000 (88.520%)\n\nstarting training..\n\nTrain Epoch: 193 [60000/60000 (100%)]\tLoss: 0.102060\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4421/5000 (88.420%)\n\nstarting training..\n\nTrain Epoch: 194 [60000/60000 (100%)]\tLoss: 0.092282\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4443/5000 (88.860%)\n\nstarting training..\n\nTrain Epoch: 195 [60000/60000 (100%)]\tLoss: 0.079511\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4445/5000 (88.900%)\n\nstarting training..\n\nTrain Epoch: 196 [60000/60000 (100%)]\tLoss: 0.067809\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4448/5000 (88.960%)\n\nstarting training..\n\nTrain Epoch: 197 [60000/60000 (100%)]\tLoss: 0.061848\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4452/5000 (89.040%)\n\nstarting training..\n\nTrain Epoch: 198 [60000/60000 (100%)]\tLoss: 0.052951\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4471/5000 (89.420%)\n\nstarting training..\n\nTrain Epoch: 199 [60000/60000 (100%)]\tLoss: 0.048208\nstarting evalation \n\n\nAverage Val Loss: 0.0000, Val Accuracy: 4475/5000 (89.500%)\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx=1\n# pred =predict(model, x_test,desc=True, topk=5,class_name_dict=id_2_label)\npred =predict(model, x_test)\n# pred =predict(model, [x_test[0]],desc=True, topk=5,class_name_dict=id_2_label)\n# pred =predict(model, [x_test[0]])\n# display_image(test_img_data[idx])\n# print(test_labels[idx],id_2_label[test_labels[idx]])\n\n","execution_count":75,"outputs":[{"output_type":"stream","text":"Totla : 5000 , correct prediciton : 4510 , incorrect : 490 \n\nAccuracy : 90.2%\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"details, match, mismatch= get_correct_prediction(id_2_label,pred,y_test)","execution_count":76,"outputs":[{"output_type":"stream","text":"Totla : 5000 , correct prediciton : 4510 , incorrect : 490 \n\nAccuracy : 90.2%\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_out_img_size(img, Filter=[None],Stride=[None],Polling=[None]):\n    if len(img.shape) > 2:\n        w,h,_= img.shape\n    else:\n        w,h =img.shape\n        \n    print('W = {}, H = {}'.format(w,h))\n        \n    for i in range(len(Filter)):\n        f = Filter[i]\n        p= Polling[i] if Polling[i] else 0\n        s = Stride[i] if Stride[i] else 1\n        \n        w = ((w - f + 2 * p)/s) +1\n        h = ((h - f + 2 * p)/s ) +1\n        print('layer : ',i+1)\n        print('new W = {}, new H = {}'.format(w,h))\n            \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calculate_out_img_size(train_img_data[0],Filter=[3,2,3,2,3,2],Stride=[0,2,0,0,0,2],Polling=[0,0,0,0,0,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}