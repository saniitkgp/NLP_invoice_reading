{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#------------------------------importing library---------------------------------------\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from bpemb import BPEmb\n",
    "import pytesseract\n",
    "import cv2\n",
    "import glob\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from timeit import default_timer as timer\n",
    "import shutil\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "import xml.etree.cElementTree as ET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"C:/Users/Sanjeev/Downloads/software/0325updated.task2train(626p)-20200414T125907Z-001/0325updated.task2train(626p)/*.txt\"\n",
    "img_path = \"C:/Users/Sanjeev/Downloads/software/0325updated.task2train(626p)-20200414T125907Z-001/0325updated.task2train(626p)/*.jpg\"\n",
    "tesseract_path = 'C:/python_lib/ocr/tesseract.exe'\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/python_lib/ocr/tesseract.exe'\n",
    "file_save_path='C:/Users/Sanjeev/Documents/Python Scripts/Dataset/label_data/'\n",
    "xml_file=\"C:/Users/Sanjeev/Documents/Python Scripts/NLP/TensorFlow/addons/data/X00016469612.xml\"\n",
    "img_save_path='C:/Users/Sanjeev/Downloads/software/0325updated.task2train(626p)-20200414T125907Z-001/data/generated_annotation/'\n",
    "text_save_path='C:/Users/Sanjeev/Downloads/software/0325updated.task2train(626p)-20200414T125907Z-001/data/generated_annotation/text_dict/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path_list = glob.glob(text_path)\n",
    "img_path_list = glob.glob(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     4,
     8,
     24,
     45,
     52
    ]
   },
   "outputs": [],
   "source": [
    "def SaveData(data,path):\n",
    "    with open(path,'wb') as file:\n",
    "        pickle.dump(data,file)\n",
    "\n",
    "def LoadData(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def display_image(img):\n",
    "    res = isinstance(img, str)\n",
    "    if res:\n",
    "        print('reading image ../n' )\n",
    "        img = cv2.imread(img)  \n",
    "    if len(img.shape) > 2:\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(\"Image\")\n",
    "        plt.axis('off')\n",
    "    else:\n",
    "        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"RGB Image\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        \n",
    "def extract_bbox_info(img_path):\n",
    "    hocr_data = pytesseract.image_to_pdf_or_hocr(img_path, extension='hocr')\n",
    "    soup = BeautifulSoup(hocr_data)\n",
    "    spans = soup.find_all('span', attrs={'class':'ocrx_word'})  # getting the attributes \n",
    "    word_bbox_list =[]  \n",
    "#     bbox_width=[]\n",
    "    for span in spans:\n",
    "        bbox_str=span['title']  # geting bounding box inforamtion \n",
    "        bbox_str =bbox_str.split(';')\n",
    "        bbox =bbox_str[0].split()[1:]\n",
    "    #     x,y,w,h= *bbox\n",
    "    #     word_info_list.append([span['id'],span.string,[bbox[1].split()[-1],*bbox]])\n",
    "        word_bbox_list.append([span['id'],span.string,\n",
    "                               [int(bbox[0]),int(bbox[1]),int(bbox[2]),int(bbox[3])]\n",
    "                              ])\n",
    "#         bbox_width.append((int(bbox[2])-int(bbox[0])))\n",
    "#         word_info_list.sort(key = lambda sort_value: (sort_value[2][1],sort_value[2][0]))\n",
    "#     word_info_list.sort(key = lambda sort_value: sort_value[2][1])\n",
    "    \n",
    "    return word_bbox_list\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \") \n",
    "# from ElementTree_pretty import prettify\n",
    "def create_xml(bbox_list=None,path=None,img_shape=None,folder='Data'):\n",
    "    w,h,d=img_shape\n",
    "    \n",
    "    root = ET.Element(\"annotation\")\n",
    "    root_c_folder = ET.SubElement(root, \"folder\").text =folder\n",
    "    root_c_filename=ET.SubElement(root, \"filename\").text =path.split('\\\\')[-1]\n",
    "    \n",
    "    root_c_path=ET.SubElement(root, \"path\").text =path\n",
    "    root_c_source=ET.SubElement(root, \"source\")\n",
    "    ET.SubElement(root_c_source, \"database\").text = \"Unknown\"\n",
    "    \n",
    "    root_c_size=ET.SubElement(root, \"size\")\n",
    "    ET.SubElement(root_c_size, \"width\").text = str(int(w))\n",
    "    ET.SubElement(root_c_size, \"height\").text = str(int(h))\n",
    "    ET.SubElement(root_c_size, \"depth\").text = str(int(d))\n",
    "    \n",
    "    root_c_segmented=ET.SubElement(root, \"segmented\").text=str(int(0))\n",
    "    \n",
    "    for bbox_ in bbox_list:\n",
    "        bbox =bbox_[-1]\n",
    "        root_c_object=ET.SubElement(root, \"object\")\n",
    "\n",
    "        ET.SubElement(root_c_object, \"name\").text = 'text'\n",
    "        ET.SubElement(root_c_object, \"pose\").text = 'Unspecified'\n",
    "        ET.SubElement(root_c_object, \"truncated\").text = str(int(0))\n",
    "        ET.SubElement(root_c_object, \"difficult\").text = str(int(0))\n",
    "        object_c_bndbox=ET.SubElement(root_c_object, \"bndbox\")\n",
    "        ET.SubElement(object_c_bndbox, \"xmin\").text = str(int(bbox[0]))\n",
    "        ET.SubElement(object_c_bndbox, \"xmin\").text = str(int(bbox[1]))\n",
    "        ET.SubElement(object_c_bndbox, \"xmin\").text = str(int(bbox[2]))\n",
    "        ET.SubElement(object_c_bndbox, \"xmin\").text = str(int(bbox[3]))\n",
    "\n",
    "    return prettify(root)\n",
    "#     tree.write(file_save_path+\"filename.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_ocr_text(img):  \n",
    "    res = isinstance(img, str)\n",
    "    if res:\n",
    "        print('reading image ../n' )\n",
    "        img = cv2.imread(img)  \n",
    "    text = pytesseract.image_to_string(img)\n",
    "#     print(text)\n",
    "    return text\n",
    "\n",
    "# read_data ()-> out data [ocr_text_list,file_name_list,word_list,dict_list,img_list,path_list]\n",
    "def read_data_xml(img_path_list, text_path_list, data_no=150):\n",
    "    word_list,dict_list,img_list,path_list,file_name_list,ocr_text_list=[],[],[],[],[],[]\n",
    "    \n",
    "    for i in range(data_no):\n",
    "        print('file : ',i, '-> remaining : ',data_no-i)\n",
    "        img_path= img_path_list[i]\n",
    "        path_list.append(img_path_list[i])\n",
    "        img_name = img_path.split(\"\\\\\")[-1]\n",
    "        img_name = img_name.split(\".jpg\")[0]\n",
    "        img_data = cv2.imread(img_path_list[i])\n",
    "        img_list.append(img_data)\n",
    "        ocr_text = get_ocr_text(img_path)\n",
    "        ocr_text_list.append(ocr_text)\n",
    "        word_list.append(extract_bbox_info(img_path_list[i]))\n",
    "        text_path = text_path_list[i]\n",
    "        text_name=text_path.split(\"\\\\\")[-1]\n",
    "        text_name = text_name.split('.txt')[0]\n",
    "        while img_name != text_name:\n",
    "            j=i \n",
    "            if img_name.split !=text_name.split(\" \")[0]:\n",
    "                break\n",
    "            else:\n",
    "\n",
    "                j +=1\n",
    "                print('fileName is not same: ',img_name, text_name)\n",
    "                text_path = text_path_list[j]\n",
    "                text_name=text_path.split(\"\\\\\")[-1]\n",
    "                text_name = text_name.split('.txt')[0]\n",
    "        with open(text_path, 'r') as file:\n",
    "            text_data =file.read()\n",
    "        dict_list.append(eval(text_data))\n",
    "        file_name_list.append(img_name)\n",
    "#         data_list.append([img_data,ocr_text,eval(text_data),img_name])    \n",
    "    return ocr_text_list,file_name_list,word_list,dict_list,img_list,path_list\n",
    "\n",
    "def read_data_yolo(img_path_list, text_path_list, data_no=150):\n",
    "    word_list,dict_list,img_list,path_list,file_name_list,ocr_text_list=[],[],[],[],[],[]\n",
    "    for i in range(data_no):\n",
    "        print('file : ',i, '-> remaining : ',data_no-i)\n",
    "        img_path= img_path_list[i]\n",
    "        path_list.append(img_path_list[i])\n",
    "        img_name = img_path.split(\"\\\\\")[-1]\n",
    "        img_name = img_name.split(\".jpg\")[0]\n",
    "        img_data = cv2.imread(img_path_list[i])\n",
    "        img_list.append(img_data)\n",
    "        ocr_text = get_ocr_text(img_path)\n",
    "        ocr_text_list.append(ocr_text)\n",
    "        word_list.append(extract_bbox_info(img_path_list[i]))\n",
    "        text_path = text_path_list[i]\n",
    "        text_name=text_path.split(\"\\\\\")[-1]\n",
    "        text_name = text_name.split('.txt')[0]\n",
    "        with open(text_path, 'r') as file:\n",
    "            text_data =file.read()\n",
    "        dict_list.append(eval(text_data))\n",
    "        file_name_list.append(img_name)\n",
    "#         data_list.append([img_data,ocr_text,eval(text_data),img_name])    \n",
    "    return ocr_text_list,file_name_list,word_list,dict_list,img_list,path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0,
     23
    ]
   },
   "outputs": [],
   "source": [
    "def Line_Formation(word_list):\n",
    "    word_list.sort(key =lambda sort_on_top : sort_on_top[2][1]) # sorting on top value \n",
    "    line_list =[]  \n",
    "    word_list_len = len(word_list)\n",
    "    temp=[]\n",
    "    temp.append(word_list[0])   # intializing the value with pre-assumption \n",
    "    for i in range(1,word_list_len, 1):\n",
    "        wa_top =word_list[i-1][2][1]\n",
    "        wa_bottom = word_list[i-1][2][3]\n",
    "\n",
    "        wb_top =word_list[i][2][1]\n",
    "        wb_bottom = word_list[i][2][3]\n",
    "        if wa_top <= wb_bottom and wa_bottom >= wb_top:\n",
    "            temp.append(word_list[i])\n",
    "        else:\n",
    "            temp.sort(key=lambda sort_on_left_ : sort_on_left_[2][0]) # sorting on the left value\n",
    "            line_list.append(temp)\n",
    "            temp=[]\n",
    "            temp.append(word_list[i])\n",
    "    line_list.append(temp)\n",
    "    return line_list\n",
    "\n",
    "# output-> arr_word_list, dict_list\n",
    "def create_output(word_list,dict_word,skip=None):\n",
    "    dict_list = []\n",
    "    for key,value in dict_word.items():\n",
    "        if skip==key:\n",
    "            continue\n",
    "        dict_list.append(value.split())\n",
    "\n",
    "        arr_word_list=[]\n",
    "    lines=Line_Formation(word_list)\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            arr_word_list.append(word)\n",
    "        \n",
    "    return arr_word_list, dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def get_index(text,dict_list):\n",
    "    for idx,word_list in enumerate(dict_list):\n",
    "        if text in word_list:\n",
    "            return idx\n",
    "    return -1\n",
    "\n",
    "def get_bbox_list(arr_word_list,dict_list):\n",
    "    bbox_list,idx =[],0\n",
    "    while idx < len(arr_word_list):\n",
    "        text= arr_word_list[idx][1]\n",
    "        value_idx = get_index(text,dict_list)\n",
    "        if value_idx ==-1:\n",
    "            idx +=1\n",
    "            continue\n",
    "#         print('value_id : ',value_idx,'idx ', idx, 'text = ', text)\n",
    "#         print(text,\"--->\", dict_list[value_idx])\n",
    "        if text in dict_list[value_idx]:     #case when 1st word matches\n",
    "            l=dict_list[value_idx].index(text) # if index is not zero\n",
    "#             print('-------> index check (',text,'): ', l )\n",
    "            j=0\n",
    "            if l != 0:\n",
    "#                 print('case when l not eaual to zero vlaue l ', l)\n",
    "                j=l\n",
    "                while l > 0:\n",
    "#                     print('appending word: ',arr_word_list[idx-l])\n",
    "                    bbox_list.append(arr_word_list[idx-l])\n",
    "                    l=l-1\n",
    "            while j < len(dict_list[value_idx]) and arr_word_list[idx][1] != dict_list[value_idx][-1]:  \n",
    "                bbox_list.append(arr_word_list[idx])  # todo what should be the output data actural data or extracted one (now). \n",
    "                idx +=1\n",
    "                j +=1\n",
    "            bbox_list.append(arr_word_list[idx])\n",
    "            del dict_list[value_idx]\n",
    "            value_idx +=1\n",
    "        idx +=1\n",
    "    return bbox_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading image ../n\n",
      "reading image ../n\n",
      "fileName is not same:  X00016469619 X00016469612\n"
     ]
    }
   ],
   "source": [
    "_,_,word_list,dict_list,img_list,path_list = read_data_xml(img_path_list, text_path_list, data_no=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xml_annotation(word_list, dict_list, img_list, path_list, file_save_path=''):\n",
    "    \n",
    "    file_len =len(word_list)\n",
    "    \n",
    "    for idx in range(file_len):\n",
    "        \n",
    "        arr_word_list, dict_text_list = create_output(word_list[idx],dict_list[idx])\n",
    "        bbox_list = get_bbox_list(arr_word_list,dict_text_list)\n",
    "\n",
    "        xml_text=create_xml(bbox_list,path_list[idx],img_list[idx].shape,folder='Data')\n",
    "        file_name = path_list[idx].split('\\\\')[-1]\n",
    "        file_name=file_name.split('.')[0]+'.xml'\n",
    "        with open(file_save_path+file_name, \"w\") as file:\n",
    "            file.write(xml_text)\n",
    "        print(xml_text,'\\n\\n---------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_xml_annotation(word_list, dict_list, img_list, path_list,file_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_data ()-> out data [image_list,ocr_text,out_text,file_name]\n",
    "def copy_data(img_path_list, text_path_list,img_save_path, data_no=150):\n",
    "    img_list,ocr_text_list,text_list,file_name_list=[],[],[],[]\n",
    "    \n",
    "    for i in range(data_no):\n",
    "        img_path= img_path_list[i]\n",
    "        img_name = img_path.split(\"\\\\\")[-1]\n",
    "        shutil.copyfile(img_path, img_save_path+img_path.split(\"\\\\\")[-1])\n",
    "        img_name=img_name.split('.')[0]\n",
    "        text_path = text_path_list[i]\n",
    "        text_name=text_path.split(\"\\\\\")[-1]\n",
    "        text_name = text_name.split('.txt')[0]\n",
    "        while img_name != text_name:\n",
    "            j=i\n",
    "            if img_name.split !=text_name.split(\" \")[0]:\n",
    "                break\n",
    "            else:\n",
    "                j +=1\n",
    "                print('fileName is not same: ',img_name, text_name)\n",
    "                text_path = text_path_list[j]\n",
    "                text_name=text_path.split(\"\\\\\")[-1]\n",
    "                text_name = text_name.split('.txt')[0]\n",
    "        print(i,img_name,text_name) \n",
    "        shutil.copyfile(text_path, text_save_path+text_path.split(\"\\\\\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 X00016469612 X00016469612\n",
      "1 X00016469619 X00016469619\n",
      "2 X00016469620 X00016469620\n",
      "3 X00016469622 X00016469622\n",
      "4 X00016469623 X00016469623\n",
      "5 X00016469669 X00016469669\n",
      "6 X00016469672 X00016469672\n",
      "7 X00016469676 X00016469676\n",
      "8 X51005200938 X51005200938\n",
      "9 X51005230617 X51005230617\n",
      "10 X51005255805 X51005255805\n",
      "11 X51005268200 X51005268200\n",
      "12 X51005268262 X51005268262\n",
      "13 X51005268400 X51005268400\n",
      "14 X51005268472 X51005268472\n",
      "15 X51005301659(1) X51005301659(1)\n",
      "16 X51005301659(2) X51005301659(2)\n",
      "17 X51005301659(3) X51005301659(3)\n",
      "18 X51005301659(4) X51005301659(4)\n",
      "19 X51005301659 X51005301659\n",
      "20 X51005301661(1) X51005301661(1)\n",
      "21 X51005301661(2) X51005301661(2)\n",
      "22 X51005301661 X51005301661(3)\n",
      "23 X51005301667(1) X51005301661(4)\n",
      "24 X51005301667(2) X51005301661\n",
      "25 X51005301667 X51005301667(1)\n",
      "26 X51005303661(1) X51005301667(2)\n",
      "27 X51005303661(2) X51005301667(3)\n",
      "28 X51005303661(3) X51005301667(4)\n",
      "29 X51005303661(4) X51005301667\n",
      "30 X51005303661 X51005303661(1)\n",
      "31 X51005306399(1) X51005303661(2)\n",
      "32 X51005306399(2) X51005303661(3)\n",
      "33 X51005306399(3) X51005303661(4)\n",
      "34 X51005306399(4) X51005303661\n",
      "35 X51005306399 X51005306399(1)\n",
      "36 X51005337872(1) X51005306399(2)\n",
      "37 X51005337872 X51005306399(3)\n",
      "38 X51005361883(1) X51005306399(4)\n",
      "39 X51005361883 X51005306399(5)\n",
      "40 X51005361895(1) X51005306399\n",
      "41 X51005361895 X51005337872(1)\n",
      "42 X51005361897(1) X51005337872(2)\n",
      "43 X51005361897 X51005337872\n",
      "44 X51005361898(1) X51005361883(1)\n",
      "45 X51005361898 X51005361883(2)\n",
      "46 X51005361900(1) X51005361883\n",
      "47 X51005361900 X51005361895(1)\n",
      "48 X51005361907 X51005361895(2)\n",
      "49 X51005361946 X51005361895\n",
      "50 X51005361950 X51005361897(1)\n",
      "51 X51005365179 X51005361897(2)\n",
      "52 X51005433492(1) X51005361897\n",
      "53 X51005433492 X51005361898(1)\n",
      "54 X51005433494 X51005361898(2)\n",
      "55 X51005433514 X51005361898\n",
      "56 X51005433522 X51005361900(1)\n",
      "57 X51005433533 X51005361900\n",
      "58 X51005433538 X51005361907(1)\n",
      "59 X51005433541 X51005361907\n",
      "60 X51005433552 X51005361946(1)\n",
      "61 X51005433553 X51005361946\n",
      "62 X51005441398 X51005361950(1)\n",
      "63 X51005441401 X51005361950\n",
      "64 X51005441402 X51005365179(1)\n",
      "65 X51005441407 X51005365179\n",
      "66 X51005441408 X51005433492\n",
      "67 X51005442327 X51005433494\n",
      "68 X51005442333 X51005433514\n",
      "69 X51005442338 X51005433522\n",
      "70 X51005442341 X51005433533\n",
      "71 X51005442344 X51005433538\n",
      "72 X51005442346 X51005433541\n",
      "73 X51005442361 X51005433552\n",
      "74 X51005442376 X51005433553\n",
      "75 X51005442378 X51005441398\n",
      "76 X51005442379 X51005441401\n",
      "77 X51005442383 X51005441402\n",
      "78 X51005442384 X51005441407\n",
      "79 X51005442386 X51005441408\n",
      "80 X51005442392 X51005442327\n",
      "81 X51005442394 X51005442333\n",
      "82 X51005442397 X51005442338\n",
      "83 X51005444033 X51005442341\n",
      "84 X51005444037 X51005442344\n",
      "85 X51005444045 X51005442346\n",
      "86 X51005447832 X51005442361\n",
      "87 X51005447833 X51005442376\n",
      "88 X51005447839 X51005442378\n",
      "89 X51005447840 X51005442379\n",
      "90 X51005447848 X51005442383\n",
      "91 X51005447850 X51005442384\n",
      "92 X51005447852 X51005442386\n",
      "93 X51005447853 X51005442392\n",
      "94 X51005447860(1) X51005442394\n",
      "95 X51005447860 X51005442397\n",
      "96 X51005447861(1) X51005444033\n",
      "97 X51005447861 X51005444037\n",
      "98 X51005453729(1) X51005444045\n",
      "99 X51005453729 X51005447832\n",
      "100 X51005453801(1) X51005447833\n",
      "101 X51005453801 X51005447839\n",
      "102 X51005453802(1) X51005447840\n",
      "103 X51005453802 X51005447848\n",
      "104 X51005453804 X51005447850\n",
      "105 X51005568827 X51005447852\n",
      "106 X51005568829 X51005447853(1)\n",
      "107 X51005568880 X51005447853\n",
      "108 X51005568881 X51005447860(1)\n",
      "109 X51005568884 X51005447860\n",
      "110 X51005568891 X51005447861(1)\n",
      "111 X51005568898 X51005447861\n",
      "112 X51005568899 X51005453729(1)\n",
      "113 X51005568900 X51005453729\n",
      "114 X51005568911 X51005453801(1)\n",
      "115 X51005568913 X51005453801\n",
      "116 X51005568914 X51005453802\n",
      "117 X51005577192 X51005453804\n",
      "118 X51005587254 X51005568827\n",
      "119 X51005587267(1) X51005568829\n",
      "120 X51005587267 X51005568880\n",
      "121 X51005605284(1) X51005568881\n",
      "122 X51005605284(2) X51005568884\n",
      "123 X51005605284 X51005568891\n",
      "124 X51005605285(1) X51005568898\n",
      "125 X51005605285(2) X51005568899\n",
      "126 X51005605285 X51005568900(1)\n",
      "127 X51005605286(1) X51005568900\n",
      "128 X51005605286 X51005568911(1)\n",
      "129 X51005605333(1) X51005568911\n",
      "130 X51005605333 X51005568913(1)\n",
      "131 X51005605334 X51005568913\n",
      "132 X51005605335 X51005568914(1)\n",
      "133 X51005663272 X51005568914\n",
      "134 X51005663273 X51005577192(1)\n",
      "135 X51005663276 X51005577192\n",
      "136 X51005663277(1) X51005587254(1)\n",
      "137 X51005663277 X51005587254(2)\n",
      "138 X51005663278 X51005587254\n",
      "139 X51005663279(1) X51005587267(1)\n",
      "140 X51005663279 X51005587267(2)\n",
      "141 X51005663280(1) X51005587267\n",
      "142 X51005663280 X51005605284(1)\n",
      "143 X51005663293 X51005605284(2)\n",
      "144 X51005663297 X51005605284\n",
      "145 X51005663298 X51005605285(1)\n",
      "146 X51005663301 X51005605285(2)\n",
      "147 X51005663311 X51005605285\n",
      "148 X51005663317 X51005605286(1)\n",
      "149 X51005663324 X51005605286(2)\n",
      "150 X51005675095 X51005605286\n",
      "151 X51005676536 X51005605333\n",
      "152 X51005676538 X51005605334\n",
      "153 X51005676539 X51005605335\n",
      "154 X51005676540(1) X51005663272\n",
      "155 X51005676540 X51005663273\n",
      "156 X51005676541(1) X51005663276(1)\n",
      "157 X51005676541 X51005663276\n",
      "158 X51005676543(1) X51005663277(1)\n",
      "159 X51005676543 X51005663277\n",
      "160 X51005676544 X51005663278(1)\n",
      "161 X51005676545 X51005663278\n",
      "162 X51005676546(1) X51005663279(1)\n",
      "163 X51005676546(2) X51005663279\n",
      "164 X51005676546 X51005663280(1)\n",
      "165 X51005676547(1) X51005663280\n",
      "166 X51005676547(2) X51005663293\n",
      "167 X51005676547 X51005663297\n",
      "168 X51005676549(1) X51005663298\n",
      "169 X51005676549(2) X51005663301\n",
      "170 X51005676549 X51005663311\n",
      "171 X51005677328(1) X51005663317\n",
      "172 X51005677328(2) X51005663324\n",
      "173 X51005677328 X51005675095\n",
      "174 X51005677329(1) X51005676536\n",
      "175 X51005677329 X51005676538\n",
      "176 X51005677331 X51005676539(1)\n",
      "177 X51005677332 X51005676539\n",
      "178 X51005677334 X51005676540(1)\n",
      "179 X51005677335 X51005676540\n",
      "180 X51005677339 X51005676541(1)\n",
      "181 X51005685355(1) X51005676541\n",
      "182 X51005685355 X51005676543(1)\n",
      "183 X51005685357(1) X51005676543\n",
      "184 X51005685357 X51005676544(1)\n",
      "185 X51005705722 X51005676544\n",
      "186 X51005705760 X51005676545(1)\n",
      "187 X51005705804(1) X51005676545(2)\n",
      "188 X51005705804 X51005676545\n",
      "189 X51005711401(1) X51005676546(1)\n",
      "190 X51005711401 X51005676546(2)\n",
      "191 X51005711403(1) X51005676546\n",
      "192 X51005711403 X51005676547(1)\n",
      "193 X51005711404(1) X51005676547(2)\n",
      "194 X51005711404 X51005676547\n",
      "195 X51005711441 X51005676549(1)\n",
      "196 X51005711442 X51005676549(2)\n",
      "197 X51005711445 X51005676549\n",
      "198 X51005711447 X51005677328(1)\n",
      "199 X51005711451 X51005677328(2)\n"
     ]
    }
   ],
   "source": [
    "copy_data(img_path_list,text_path_list,img_save_path,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 200\n"
     ]
    }
   ],
   "source": [
    "text_path_list = glob.glob(text_save_path + '*.txt')\n",
    "img_path_list = glob.glob(img_save_path+'*.jpg')\n",
    "print(len(text_path_list),len(text_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n",
      "reading image ../n\n"
     ]
    }
   ],
   "source": [
    "_,_,word_list,dict_list,img_list,path_list = read_data_yolo(img_path_list, text_path_list, data_no=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yolo_annotation(word_list, dict_list, img_list, path_list, file_save_path=''):\n",
    "    \n",
    "    file_len =len(word_list)\n",
    "    class_id =0\n",
    "    \n",
    "    for idx in range(file_len):\n",
    "        print('file : ',idx, '-> remaining : ',200-idx)\n",
    "        arr_word_list, dict_text_list = create_output(word_list[idx],dict_list[idx],skip='total')\n",
    "        bbox_list = get_bbox_list(arr_word_list,dict_text_list)\n",
    "        X,Y,_ =img_list[idx].shape\n",
    "        file_name = path_list[idx].split(\"\\\\\")[-1]\n",
    "        file_name=file_name.split('.')[0]\n",
    "#         print(file_save_path +file_name+'.txt')\n",
    "        with open(file_save_path +file_name+'.txt', 'w') as file:\n",
    "            \n",
    "            for bbox in bbox_list:\n",
    "                x,y,w,h=bbox[-1]\n",
    "                x0=round((x+(w-x)/2)/Y, 6)\n",
    "                y0=round((y+(h-y)/2)/X, 6)\n",
    "                W=round((w-x)/Y, 6)\n",
    "                H=round((h-y)/X, 6)\n",
    "                file.writelines(str(class_id)+' '+str(x0)+' '+str(y0)+' '+str(W)+' '+str(H)+'\\n')\n",
    "            #             print('x={},y={},W={},h={}'.format(bbox[-1][0],bbox[-1][1],bbox[-1][2],bbox[-1][3]))\n",
    "#             print('w-x(x2)={},h-y(y2)={}'.format(bbox[-1][2] - bbox[-1][0], bbox[-1][3]- bbox[-1][1]))\n",
    "#             print('u-> w-x(x2)={},h-y(y2)={}'.format(bbox[-1][0] - bbox[-1][2], bbox[-1][1]- bbox[-1][3]))\n",
    "# #             print('w-x(W)={},h-y(H)={}'.format((bbox[-1][2] - bbox[-1][0])/X, (bbox[-1][3]- bbox[-1][1]))/Y)\n",
    "#             print('{} {} {} {} {}'.format(class_id,x0,y0,W,H)) \n",
    "#             break\n",
    "#         print('\\n-------------next\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file :  0 -> remaining :  200\n",
      "file :  1 -> remaining :  199\n",
      "file :  2 -> remaining :  198\n",
      "file :  3 -> remaining :  197\n",
      "file :  4 -> remaining :  196\n",
      "file :  5 -> remaining :  195\n",
      "file :  6 -> remaining :  194\n",
      "file :  7 -> remaining :  193\n",
      "file :  8 -> remaining :  192\n",
      "file :  9 -> remaining :  191\n",
      "file :  10 -> remaining :  190\n",
      "file :  11 -> remaining :  189\n",
      "file :  12 -> remaining :  188\n",
      "file :  13 -> remaining :  187\n",
      "file :  14 -> remaining :  186\n",
      "file :  15 -> remaining :  185\n",
      "file :  16 -> remaining :  184\n",
      "file :  17 -> remaining :  183\n",
      "file :  18 -> remaining :  182\n",
      "file :  19 -> remaining :  181\n",
      "file :  20 -> remaining :  180\n",
      "file :  21 -> remaining :  179\n",
      "file :  22 -> remaining :  178\n",
      "file :  23 -> remaining :  177\n",
      "file :  24 -> remaining :  176\n",
      "file :  25 -> remaining :  175\n",
      "file :  26 -> remaining :  174\n",
      "file :  27 -> remaining :  173\n",
      "file :  28 -> remaining :  172\n",
      "file :  29 -> remaining :  171\n",
      "file :  30 -> remaining :  170\n",
      "file :  31 -> remaining :  169\n",
      "file :  32 -> remaining :  168\n",
      "file :  33 -> remaining :  167\n",
      "file :  34 -> remaining :  166\n",
      "file :  35 -> remaining :  165\n",
      "file :  36 -> remaining :  164\n",
      "file :  37 -> remaining :  163\n",
      "file :  38 -> remaining :  162\n",
      "file :  39 -> remaining :  161\n",
      "file :  40 -> remaining :  160\n",
      "file :  41 -> remaining :  159\n",
      "file :  42 -> remaining :  158\n",
      "file :  43 -> remaining :  157\n",
      "file :  44 -> remaining :  156\n",
      "file :  45 -> remaining :  155\n",
      "file :  46 -> remaining :  154\n",
      "file :  47 -> remaining :  153\n",
      "file :  48 -> remaining :  152\n",
      "file :  49 -> remaining :  151\n",
      "file :  50 -> remaining :  150\n",
      "file :  51 -> remaining :  149\n",
      "file :  52 -> remaining :  148\n",
      "file :  53 -> remaining :  147\n",
      "file :  54 -> remaining :  146\n",
      "file :  55 -> remaining :  145\n",
      "file :  56 -> remaining :  144\n",
      "file :  57 -> remaining :  143\n",
      "file :  58 -> remaining :  142\n",
      "file :  59 -> remaining :  141\n",
      "file :  60 -> remaining :  140\n",
      "file :  61 -> remaining :  139\n",
      "file :  62 -> remaining :  138\n",
      "file :  63 -> remaining :  137\n",
      "file :  64 -> remaining :  136\n",
      "file :  65 -> remaining :  135\n",
      "file :  66 -> remaining :  134\n",
      "file :  67 -> remaining :  133\n",
      "file :  68 -> remaining :  132\n",
      "file :  69 -> remaining :  131\n",
      "file :  70 -> remaining :  130\n",
      "file :  71 -> remaining :  129\n",
      "file :  72 -> remaining :  128\n",
      "file :  73 -> remaining :  127\n",
      "file :  74 -> remaining :  126\n",
      "file :  75 -> remaining :  125\n",
      "file :  76 -> remaining :  124\n",
      "file :  77 -> remaining :  123\n",
      "file :  78 -> remaining :  122\n",
      "file :  79 -> remaining :  121\n",
      "file :  80 -> remaining :  120\n",
      "file :  81 -> remaining :  119\n",
      "file :  82 -> remaining :  118\n",
      "file :  83 -> remaining :  117\n",
      "file :  84 -> remaining :  116\n",
      "file :  85 -> remaining :  115\n",
      "file :  86 -> remaining :  114\n",
      "file :  87 -> remaining :  113\n",
      "file :  88 -> remaining :  112\n",
      "file :  89 -> remaining :  111\n",
      "file :  90 -> remaining :  110\n",
      "file :  91 -> remaining :  109\n",
      "file :  92 -> remaining :  108\n",
      "file :  93 -> remaining :  107\n",
      "file :  94 -> remaining :  106\n",
      "file :  95 -> remaining :  105\n",
      "file :  96 -> remaining :  104\n",
      "file :  97 -> remaining :  103\n",
      "file :  98 -> remaining :  102\n",
      "file :  99 -> remaining :  101\n",
      "file :  100 -> remaining :  100\n",
      "file :  101 -> remaining :  99\n",
      "file :  102 -> remaining :  98\n",
      "file :  103 -> remaining :  97\n",
      "file :  104 -> remaining :  96\n",
      "file :  105 -> remaining :  95\n",
      "file :  106 -> remaining :  94\n",
      "file :  107 -> remaining :  93\n",
      "file :  108 -> remaining :  92\n",
      "file :  109 -> remaining :  91\n",
      "file :  110 -> remaining :  90\n",
      "file :  111 -> remaining :  89\n",
      "file :  112 -> remaining :  88\n",
      "file :  113 -> remaining :  87\n",
      "file :  114 -> remaining :  86\n",
      "file :  115 -> remaining :  85\n",
      "file :  116 -> remaining :  84\n",
      "file :  117 -> remaining :  83\n",
      "file :  118 -> remaining :  82\n",
      "file :  119 -> remaining :  81\n",
      "file :  120 -> remaining :  80\n",
      "file :  121 -> remaining :  79\n",
      "file :  122 -> remaining :  78\n",
      "file :  123 -> remaining :  77\n",
      "file :  124 -> remaining :  76\n",
      "file :  125 -> remaining :  75\n",
      "file :  126 -> remaining :  74\n",
      "file :  127 -> remaining :  73\n",
      "file :  128 -> remaining :  72\n",
      "file :  129 -> remaining :  71\n",
      "file :  130 -> remaining :  70\n",
      "file :  131 -> remaining :  69\n",
      "file :  132 -> remaining :  68\n",
      "file :  133 -> remaining :  67\n",
      "file :  134 -> remaining :  66\n",
      "file :  135 -> remaining :  65\n",
      "file :  136 -> remaining :  64\n",
      "file :  137 -> remaining :  63\n",
      "file :  138 -> remaining :  62\n",
      "file :  139 -> remaining :  61\n",
      "file :  140 -> remaining :  60\n",
      "file :  141 -> remaining :  59\n",
      "file :  142 -> remaining :  58\n",
      "file :  143 -> remaining :  57\n",
      "file :  144 -> remaining :  56\n",
      "file :  145 -> remaining :  55\n",
      "file :  146 -> remaining :  54\n",
      "file :  147 -> remaining :  53\n",
      "file :  148 -> remaining :  52\n",
      "file :  149 -> remaining :  51\n",
      "file :  150 -> remaining :  50\n",
      "file :  151 -> remaining :  49\n",
      "file :  152 -> remaining :  48\n",
      "file :  153 -> remaining :  47\n",
      "file :  154 -> remaining :  46\n",
      "file :  155 -> remaining :  45\n",
      "file :  156 -> remaining :  44\n",
      "file :  157 -> remaining :  43\n",
      "file :  158 -> remaining :  42\n",
      "file :  159 -> remaining :  41\n",
      "file :  160 -> remaining :  40\n",
      "file :  161 -> remaining :  39\n",
      "file :  162 -> remaining :  38\n",
      "file :  163 -> remaining :  37\n",
      "file :  164 -> remaining :  36\n",
      "file :  165 -> remaining :  35\n",
      "file :  166 -> remaining :  34\n",
      "file :  167 -> remaining :  33\n",
      "file :  168 -> remaining :  32\n",
      "file :  169 -> remaining :  31\n",
      "file :  170 -> remaining :  30\n",
      "file :  171 -> remaining :  29\n",
      "file :  172 -> remaining :  28\n",
      "file :  173 -> remaining :  27\n",
      "file :  174 -> remaining :  26\n",
      "file :  175 -> remaining :  25\n",
      "file :  176 -> remaining :  24\n",
      "file :  177 -> remaining :  23\n",
      "file :  178 -> remaining :  22\n",
      "file :  179 -> remaining :  21\n",
      "file :  180 -> remaining :  20\n",
      "file :  181 -> remaining :  19\n",
      "file :  182 -> remaining :  18\n",
      "file :  183 -> remaining :  17\n",
      "file :  184 -> remaining :  16\n",
      "file :  185 -> remaining :  15\n",
      "file :  186 -> remaining :  14\n",
      "file :  187 -> remaining :  13\n",
      "file :  188 -> remaining :  12\n",
      "file :  189 -> remaining :  11\n",
      "file :  190 -> remaining :  10\n",
      "file :  191 -> remaining :  9\n",
      "file :  192 -> remaining :  8\n",
      "file :  193 -> remaining :  7\n",
      "file :  194 -> remaining :  6\n",
      "file :  195 -> remaining :  5\n",
      "file :  196 -> remaining :  4\n",
      "file :  197 -> remaining :  3\n",
      "file :  198 -> remaining :  2\n",
      "file :  199 -> remaining :  1\n"
     ]
    }
   ],
   "source": [
    "create_yolo_annotation(word_list, dict_list, img_list, path_list,img_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(52/463)\n",
    "print((109-97)/1013)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "xml_f=\"C:/Users/Sanjeev/Documents/Python Scripts/Dataset/label_data/X00016469612.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "\n",
    "# parse an xml file by name\n",
    "mydoc = minidom.parse(xml_f)\n",
    "\n",
    "items = mydoc.getElementsByTagName('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(xml_f)\n",
    "root = tree.getroot()\n",
    "print('\\nAll item data:')\n",
    "for elem in root:\n",
    "    for subelem in elem:\n",
    "        print(subelem.text)\n",
    "        for bbox in subelem:\n",
    "            print('bbox :-> ',bbox.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path_list = glob.glob(text_path)\n",
    "img_path_list = glob.glob(img_path)\n",
    "print(len(text_path_list) , len(img_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_path_list)):\n",
    "    if img_path_list[i].split(\"\\\\\")[0] == text_path_list[i].split(\"\\\\\")[0]:\n",
    "        continue\n",
    "    else:\n",
    "        print('data is not same for : ',i ,\" \\n\",img_path_list[i].split(\"\\\\\")[0],\" \\n\",text_path_list[i].split(\"\\\\\")[0])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trevaller_policy",
   "language": "python",
   "name": "trevaller_policy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
