{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------importing library-----------------------------------------\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "import pickle\n",
    "from bpemb import BPEmb\n",
    "import pytesseract\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch.utils.data import Dataset \n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "from torch_geometric.utils import convert\n",
    "\n",
    "import torch.nn as nn\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     4,
     11,
     19,
     36,
     45,
     54,
     63,
     73,
     76,
     79,
     82,
     93,
     100,
     108,
     119,
     135,
     152,
     171,
     178,
     192,
     213,
     228,
     248,
     270,
     286,
     294,
     303,
     346,
     354,
     395,
     440,
     453,
     464,
     499,
     513,
     534,
     553,
     593,
     639,
     696,
     705,
     714,
     740
    ]
   },
   "outputs": [],
   "source": [
    "def SaveData(data,path):\n",
    "    with open(path,'wb') as file:\n",
    "        pickle.dump(data,file)\n",
    "\n",
    "def LoadData(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "    \n",
    "# boolean_feature =['isDate','isZipCode','isKnownCity','isKnownDept','isKnownCountry', # below is nature feature\n",
    "# 'isAlphabetic','isNumeric','isAlphaNumeric','isNumberwithDecimal','isRealNumber','isCurrency','hasRealandCurrency','mix/mixc']\n",
    "\n",
    "def is_date_o(string, fuzzy=False):\n",
    "    print(string)\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def is_date(string, fuzzy=False):\n",
    "#     print(string)\n",
    "    try:\n",
    "        if is_number(string):\n",
    "            if(int(string) > 9999):\n",
    "                return False\n",
    "        elif is_numberwithDecimal(string):\n",
    "            return False     \n",
    "    except:\n",
    "        if is_numberwithDecimal(string):\n",
    "            return False\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def is_zipcode(code_str,database):\n",
    "    if is_number(code_str):\n",
    "        if code_str in database:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else :\n",
    "        return False\n",
    "    \n",
    "def is_knowCity(city_str,database):\n",
    "    if is_alphbetic(city_str):\n",
    "        if city_str in database:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "def is_knownDept(dept_str,database):\n",
    "    if is_alphbetic(dept_str):\n",
    "        if dept_str in database:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "def is_knownCountry(country_str,database):\n",
    "    if is_alphbetic(country_str):\n",
    "        if country_str in database:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else :\n",
    "        return False    \n",
    "    \n",
    "# creatring fucntion for nature vector\n",
    "def is_alphbetic(alph_str):\n",
    "    return bool(re.match('^[a-zA-Z]+$', alph_str))\n",
    "\n",
    "def is_number(num_str):\n",
    "    return bool(re.match('^[0-9]+$', num_str))\n",
    "\n",
    "def is_alphaNumeric(alphNum_str):\n",
    "    return bool(re.match('^[a-zA-Z0-9]+$', alphNum_str))\n",
    "\n",
    "def is_numberwithDecimal(num_str):\n",
    "    \n",
    "    if is_realNumer(num_str):  # checking if it is in numeric form\n",
    "        num = num_str.split('.') \n",
    "        if len(num)>=2 and len(num[-1])>=1: # length after decimal point must be >=1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:   \n",
    "        return False\n",
    "\n",
    "def is_realNumer(num_str):\n",
    "    try: \n",
    "        float(num_str)\n",
    "    except ValueError: \n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "def is_currency(curr_str):\n",
    "    curr_symbol='¥$€£₹'\n",
    "    curr_str =list(curr_str)\n",
    "    if curr_str[0] in curr_symbol or curr_str[-1] in curr_symbol:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_hasRealandCurrency(curr_str):\n",
    "    currency = list(curr_str)\n",
    "    curr_nature ='-+'\n",
    "    if currency[0] in curr_nature:\n",
    "        if is_currency(''.join(currency[1:])):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "def create_nature_vector(word):\n",
    "    nature=[] # 8 -dimentional vector \n",
    "    nature.append(is_alphbetic(word))\n",
    "    nature.append(is_number(word))\n",
    "    nature.append(is_alphaNumeric(word))\n",
    "    nature.append(is_numberwithDecimal(word))\n",
    "    nature.append(is_realNumer(word))\n",
    "    nature.append(is_currency(word))\n",
    "    nature.append(is_hasRealandCurrency(word))    \n",
    "    if not any(nature):\n",
    "        mix =True\n",
    "    else:\n",
    "        mix =False\n",
    "    nature.append(mix)\n",
    "    return nature \n",
    "\n",
    "def create_Boolean_feature (word,database):\n",
    "    # boolean_feature =['isDate','isZipCode','isKnownCity','isKnownDept','isKnownCountry', # below is nature feature\n",
    "    # 'isAlphabetic','isNumeric','isAlphaNumeric','isNumberwithDecimal','isRealNumber',\n",
    "    #'isCurrency','hasRealandCurrency','mix/mixc']\n",
    "\n",
    "    date = is_date(word)\n",
    "    zipcode = is_zipcode(word,database[0])\n",
    "    city = is_knowCity(word,database[1])\n",
    "    dept = is_knownDept(word,database[2])\n",
    "    country = is_knownCountry(word,database[3])\n",
    "    nature_vect = create_nature_vector(word) \n",
    "#    print(['isDate','isZipCode','isKnownCity','isKnownDept','isKnownCountry', # below is nature feature\n",
    "#           'isAlphabetic','isNumeric','isAlphaNumeric','isNumberwithDecimal','isRealNumber',\n",
    "#    'isCurrency','hasRealandCurrency','mix/mixc'])\n",
    "    return np.array([date,zipcode,city,dept,country]+nature_vect)\n",
    "\n",
    "#https://github.com/bheinzerling/bpemb/blob/master/bpemb/bpemb.py\n",
    "def create_text_feature(word, bpemb_encoder):\n",
    "    ids = bpemb_encoder.encode_ids(word)\n",
    "#    print(ids)\n",
    "    if len(ids)==1:\n",
    "        embding = bpemb_encoder.vectors[ids]\n",
    "        embding =np.concatenate((embding.flatten(), np.zeros((200))))\n",
    "    elif len(ids)==2:\n",
    "        embding = bpemb_encoder.vectors[ids]\n",
    "        embding =np.concatenate((embding.flatten(), np.zeros((100))))\n",
    "    elif len(ids)==3:\n",
    "        embding = bpemb_encoder.vectors[ids]\n",
    "        embding=embding.flatten()\n",
    "    else:\n",
    "#        embding = bpemb_encoder.vectors[ids]  # need to take care of case having subword length >3\n",
    "#        embding = np.split(embding, 4)[0:3]\n",
    "         embding = np.zeros((300))   \n",
    "#    print(len(embding))\n",
    "    return embding\n",
    "\n",
    "def details(data):\n",
    "    print('data type :',type(data))\n",
    "    try:\n",
    "        print('data shape : ', data.shape)\n",
    "    except:\n",
    "        print('data lenght : ',len(data))\n",
    "\n",
    "def create_feature_vector(word_dict,num_feature):\n",
    "     database = LoadData(database_path) #todo \n",
    "     bpemb_encoder = BPEmb(lang=\"en\", dim=100)\n",
    "     feature_vector=[]\n",
    "     for idx, word_info in word_dict.values():\n",
    "#         print(word_info)\n",
    "        word = word_info[1].strip()\n",
    "        bool_feature = create_Boolean_feature(word,database)\n",
    "        text_feature = create_text_feature(word,bpemb_encoder)\n",
    "        feature =np.concatenate((bool_feature , num_feature[idx] , text_feature))\n",
    "        feature_vector.append(feature)\n",
    "     \n",
    "     return feature_vector\n",
    "    \n",
    "def extract_bbox_info(img_path):\n",
    "    hocr_data = pytesseract.image_to_pdf_or_hocr(img_path, extension='hocr')\n",
    "    soup = BeautifulSoup(hocr_data)\n",
    "    spans = soup.find_all('span', attrs={'class':'ocrx_word'})  # getting the attributes \n",
    "    word_bbox_list =[]  \n",
    "    bbox_width=[]\n",
    "    for span in spans:\n",
    "        bbox_str=span['title']  # geting bounding box inforamtion \n",
    "        bbox_str =bbox_str.split(';')\n",
    "        bbox =bbox_str[0].split()[1:]\n",
    "    #     x,y,w,h= *bbox\n",
    "    #     word_info_list.append([span['id'],span.string,[bbox[1].split()[-1],*bbox]])\n",
    "        word_bbox_list.append([span['id'],span.string,\n",
    "                               [int(bbox[0]),int(bbox[1]),int(bbox[2]),int(bbox[3])]\n",
    "                              ])\n",
    "        bbox_width.append((int(bbox[2])-int(bbox[0])))\n",
    "#         word_info_list.sort(key = lambda sort_value: (sort_value[2][1],sort_value[2][0]))\n",
    "#     word_info_list.sort(key = lambda sort_value: sort_value[2][1])\n",
    "    \n",
    "    return word_bbox_list, bbox_width\n",
    "\n",
    "def remove_noise(bbox_width, word_list):\n",
    "    mean=np.mean(bbox_width) # mean of the width of bounding box\n",
    "    sd=np.std(bbox_width)  #stanered deviation of the width of the bounding box\n",
    "    sd=round(sd) #Roinding to integer value and find the threshold value\n",
    "#     print(mean,sd)\n",
    "    def noise_removal_finction(box_info):\n",
    "        box_width = box_info[2][2] - box_info[2][0]\n",
    "#         print(box_width)\n",
    "        if  (mean*2-sd) <=box_width < mean+sd:\n",
    "            if box_info[1] !=' ' and box_info[1] !='-':\n",
    "                return box_width\n",
    "    \n",
    "    filtered_word_list = filter(noise_removal_finction, word_list)\n",
    "    return list(filtered_word_list)\n",
    "\n",
    "def remove_noise_info(bbox_width, word_list):\n",
    "    mean=np.mean(bbox_width) # mean of the width of bounding box\n",
    "    sd=np.std(bbox_width)  #stanered deviation of the width of the bounding box\n",
    "    sd=round(sd) #Roinding to integer value and find the threshold value\n",
    "#     print(mean,sd)\n",
    "    filtered_word_list=[]\n",
    "    remove_word_list=[]\n",
    "    for box_info in word_list:\n",
    "        box_width = box_info[2][2] - box_info[2][0]\n",
    "#         print(box_width)\n",
    "        if  (mean*2-sd) <=box_width < mean+sd:\n",
    "            if box_info[1] !=' ':\n",
    "                filtered_word_list.append(box_info)\n",
    "            else:\n",
    "                remove_word_list.append(box_info)\n",
    "        else:\n",
    "            remove_word_list.append(box_info)\n",
    "    \n",
    "    return remove_word_list, filtered_word_list\n",
    "\n",
    "def Line_Formation(word_list):\n",
    "    word_list.sort(key =lambda sort_on_top : sort_on_top[2][1]) # sorting on top value \n",
    "    line_list =[]  \n",
    "    word_list_len = len(word_list)\n",
    "    temp=[]\n",
    "    temp.append(word_list[0])   # intializing the value with pre-assumption \n",
    "    for i in range(1,word_list_len, 1):\n",
    "        wa_top =word_list[i-1][2][1]\n",
    "        wa_bottom = word_list[i-1][2][3]\n",
    "\n",
    "        wb_top =word_list[i][2][1]\n",
    "        wb_bottom = word_list[i][2][3]\n",
    "        if wa_top <= wb_bottom and wa_bottom >= wb_top:\n",
    "            temp.append(word_list[i])\n",
    "        else:\n",
    "            temp.sort(key=lambda sort_on_left_ : sort_on_left_[2][0]) # sorting on the left value\n",
    "            line_list.append(temp)\n",
    "            temp=[]\n",
    "            temp.append(word_list[i])\n",
    "    line_list.append(temp)\n",
    "    return tuple(line_list)\n",
    "       \n",
    "def get_image_data(folder_path,file_name=False):\n",
    "    file_path = glob.glob(folder_path)\n",
    "    img_list =[]\n",
    "    file_name=[]\n",
    "    for path in file_path:\n",
    "        if path.split('.')[-1]=='pdf':\n",
    "            print('pdf file skiped')\n",
    "            continue\n",
    "#        print(path)\n",
    "        img = cv2.imread(path)\n",
    "        img_list.append(img)\n",
    "        file_name.append(path)\n",
    "    if file_name:\n",
    "        return img_list,file_name\n",
    "    return img_list\n",
    "\n",
    "def display_image(img):\n",
    "    res = isinstance(img, str)\n",
    "    if res:\n",
    "        img = cv2.imread(img)\n",
    "        \n",
    "    cv2.namedWindow('Image', cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('Image', img)\n",
    "\n",
    "def display_bounding_box(img, word_bbox_list,title=\"word bounding box\"):\n",
    "    for bbox in word_bbox_list:\n",
    "        b = bbox[-1]\n",
    "        img = cv2.rectangle(img, (int(b[0]), int(b[1])), (int(b[2]), int(b[3])), (0, 255, 0), 2)\n",
    "\n",
    "    # show annotated image and wait for keypress\n",
    "    cv2.namedWindow(title, cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(title, img)\n",
    "\n",
    "def display_connected_graph(img,lookup,conn_type=None,isbox=False):\n",
    "    \n",
    "    for key in lookup.keys():    \n",
    "        if isbox or conn_type==None: \n",
    "             b =  word_dict[key][1][2]\n",
    "             img = cv2.rectangle(img, (int(b[0]), int(b[1])), (int(b[2]), int(b[3])), (0, 255, 0), 2)\n",
    "             if conn_type==None :\n",
    "                 continue\n",
    "        if conn_type=='right' or conn_type=='merge': \n",
    "           \n",
    "            if lookup.get(key).get('right') == None:\n",
    "                continue    \n",
    "            right_value = lookup[key]['right'][0]\n",
    "            right_source = word_dict[key][1][2]\n",
    "            right_target= word_dict[right_value][1][2] \n",
    "            y1 =sorted([right_source[1],right_source[3],right_target[1]])[1]\n",
    "            y2 =sorted([right_source[1],right_source[3],right_target[3]])[1]\n",
    "            y=round((y1+y2)/2)\n",
    "            start_point = (right_source[2],y)\n",
    "            end_point = (right_target[0],y)\n",
    "#            print(start_point,end_point)\n",
    "            img = cv2.line(img, (start_point),(end_point), (0, 0, 255), 2)\n",
    "            \n",
    "        if conn_type=='bottom'or conn_type=='merge':\n",
    "            if lookup.get(key).get('bottom') == None:\n",
    "                continue\n",
    "            bottom_value = lookup[key]['bottom'][0]\n",
    "            bottom_source = word_dict[key][1][2]\n",
    "            bottom_target= word_dict[bottom_value][1][2]\n",
    "#            print('bottom_source {} target {}  point stored {}'.format(bottom_source, bottom_target,word_dict[key][1]))\n",
    "            x1 =sorted([bottom_source[0],bottom_target[0],bottom_target[2]])[1]\n",
    "            x2 =sorted([bottom_source[2],bottom_target[0],bottom_target[2]])[1]\n",
    "            x=round((x1+x2)/2)\n",
    "            start_point = (x,bottom_source[3])\n",
    "            end_point = (x,bottom_target[1])\n",
    "#            print(start_point,end_point)\n",
    "            img = cv2.line(img, (start_point),(end_point), (0, 255, 255), 2)\n",
    "        \n",
    "            \n",
    "    cv2.namedWindow('connected node Image', cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('connected node Image', img)\n",
    "    cv2.waitKey(0)\n",
    "            \n",
    "def display_graph(adjacency_matrix, mylabels):\n",
    "    rows, cols = np.where(adjacency_matrix == 1)\n",
    "    edges = zip(rows.tolist(), cols.tolist())\n",
    "    gr = nx.Graph()\n",
    "    gr.add_edges_from(edges)\n",
    "    nx.draw(gr, node_size=500, labels=mylabels, with_labels=True)\n",
    "    plt.show()\n",
    "\n",
    "def find_right_NN(row,col,lines,page_width,lookup):\n",
    "    min_dist =99999\n",
    "    RD_R=99999\n",
    "    col_size = len(lines[row])\n",
    "    source =lines[row][col]\n",
    "#    print('source : ', source)\n",
    "    source_TR= source[2][1]   # assume the co-ordinates as (y,x) and bounding box has (y,x,h,w) ->(left, top, right ,bottom)\n",
    "    source_BR = source[2][3]\n",
    "    for j in range(col+1, col_size):\n",
    "        target = lines[row][j]\n",
    "        target_TL = target[2][1]\n",
    "        target_BL= target[2][3]\n",
    "        \n",
    "        case1 =source_TR <= target_TL <= source_BR       # case 1: when target top overlap with source  (horizontally)\n",
    "        case2 =source_TR <= target_BL<= source_BR # case 2: when target bottom overlap with source in right\n",
    "        case3 =target_TL < source_TR < target_BL     # case 3: when source is under overlap with target\n",
    "        if case1 or case2 or case3:\n",
    "            # calculaing relative distance WRT soruce and targe \n",
    "            #RD_R = left(targe)-right(soruce)/page_width\n",
    "            RD_R = (target[2][0] - source[2][2])/page_width\n",
    "            if(min_dist > RD_R):   # tracking minimum distance \n",
    "                if not bool(lookup.get(target[0]).get('left'))   :  # if target is empty for source in lookup \n",
    "                    lookup[source[0]]['right']=[target[0],RD_R]\n",
    "                    lookup[target[0]]['left']= [source[0],RD_R]\n",
    "                    min_dist=RD_R\n",
    "#                    print('target : ', target)\n",
    "                else:\n",
    "                    if lookup[target[0]]['left'][1] > RD_R:   # if target is Not empty for source then check left distance\n",
    "                        remove_source = lookup[target[0]]['left'][0]\n",
    "                        lookup[target[0]]['left']= [source[0],min_dist]\n",
    "                        lookup[remove_source]['right']=None\n",
    "                        min_dist=RD_R\n",
    "#                        print('update target : ',target)\n",
    "#                        print('update remove_source : ',remove_source)\n",
    "                        \n",
    "                \n",
    "        if RD_R > min_dist: # tracking prev_dist for stoping the seraching of NN  \n",
    "            break       \n",
    "    \n",
    "    return lookup\n",
    "\n",
    "def find_bottom_NN(row,col,lines,page_hight,lookup):\n",
    "    min_dist =99999\n",
    "    RD_B=99999\n",
    "    row_size = len(lines)\n",
    "    source =lines[row][col]\n",
    "#    print('source : ',source)\n",
    "    source_BL = source[2][0]  # assume the co-ordinates as (y,x) and bounding box has (y,x,h,w) ->(left, top, right ,bottom)\n",
    "    source_BR = source[2][2]\n",
    "    for i in range(row+1,row_size):\n",
    "        col_size = len(lines[i]) # finding the column of the line\n",
    "        for j in range(0, col_size):       \n",
    "            target = lines[i][j]           \n",
    "            target_TL = target[2][0]\n",
    "            target_TR = target[2][2]\n",
    "            case1 =source_BL <= target_TR <= source_BR  # case 1. target right overlap with source in bottom (vertically)\n",
    "            case2 =source_BL <= target_TL<=source_BR   # case 2. target left overlap with source  in bottom\n",
    "            case3 =target_TL < source_BL < target_TR   #case 3 . source overlap under target  in bottom\n",
    "            \n",
    "            if case1 or case2 or case3:    \n",
    "                # calculaing relative distance WRT soruce and targe \n",
    "                #RD_B = top(targe)-bottom(soruce)/page_hight\n",
    "                RD_B = (target[2][1] - source[2][3])/page_hight               \n",
    "                if(min_dist > RD_B):   # tracking minimum distance \n",
    "                    if not bool(lookup.get(target[0]).get('top')) :    # if source is empty than updated in lookup \n",
    "                        min_dist=RD_B\n",
    "                        lookup[source[0]]['bottom']=[target[0],min_dist]\n",
    "                        lookup[target[0]]['top']=[source[0],min_dist]\n",
    "#                        print('target : ',target)\n",
    "                        \n",
    "                    else:\n",
    "                        if(lookup[target[0]]['top'][1] > RD_B):   # source is updated and checking which one having less min_dist\n",
    "                            min_dist=RD_B\n",
    "                            remove_source = lookup[target[0]]['top'][0]\n",
    "                            lookup[target[0]]['top']=[source[0],min_dist]\n",
    "                            lookup[remove_source]['bottom']=None\n",
    "#                            print('update target : ',target)\n",
    "#                            print('update remove_source : ',remove_source)\n",
    "                        \n",
    "            if RD_B > min_dist : # tracking prev_dist for stoping the seraching of NN\n",
    "                i= row_size + 1 # terminating the seraching of NN in bottom \n",
    "#                 print('breaking')\n",
    "                break\n",
    "                \n",
    "    return lookup\n",
    "\n",
    "def get_NN_lookup_table(lines,page_width,page_hight):\n",
    "    \n",
    "    lookup={}\n",
    "    for row,line in  enumerate(lines):\n",
    "        for col,word in  enumerate(line):\n",
    "            lookup[word[0]]={}\n",
    "\n",
    "    for line_idx, line in enumerate(lines):    \n",
    "        for word_idx, word in enumerate(line): \n",
    "            lookup= find_right_NN(line_idx,word_idx,lines,page_width,lookup) \n",
    "            lookup= find_bottom_NN(line_idx,word_idx,lines,page_hight,lookup)\n",
    "    return lookup\n",
    "\n",
    "def get_word_dict(lines):    \n",
    "    word_dict ={}\n",
    "    contain_dict={}\n",
    "    word_count=0\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            word_dict[word[0]]=[word_count,word]\n",
    "            contain_dict[word[0]]=word[1]\n",
    "            word_count+=1\n",
    "    return word_dict,contain_dict\n",
    "\n",
    "def create_graph_numeric_feature(word_dict,lookup):\n",
    "    word_id_list = word_dict.keys()\n",
    "    size = len(word_id_list)\n",
    "    adj_mat = np.zeros((size,size))\n",
    "    num_feature = np.zeros((size,4))\n",
    "    \n",
    "    for row, word_id in enumerate(word_id_list):\n",
    "        word_NN = lookup[word_id]\n",
    "        \n",
    "        if bool(word_NN.get('left')):\n",
    "            idx , weight = word_NN.get('left')\n",
    "            col = word_dict[idx][0]\n",
    "            adj_mat[row][col]= -weight\n",
    "            num_feature[row][0]=-weight\n",
    "        \n",
    "        if bool(word_NN.get('top')):\n",
    "            idx , weight = word_NN.get('top')\n",
    "            col = word_dict[idx][0]\n",
    "            adj_mat[row][col]= -weight\n",
    "            num_feature[row][1]=-weight\n",
    "        \n",
    "        if bool(word_NN.get('right')):\n",
    "            idx , weight = word_NN.get('right')\n",
    "            col = word_dict[idx][0]\n",
    "            adj_mat[row][col]= weight\n",
    "            num_feature[row][2]=weight\n",
    "            \n",
    "        if bool(word_NN.get('bottom')):\n",
    "            idx , weight = word_NN.get('bottom')\n",
    "            col = word_dict[idx][0]\n",
    "            adj_mat[row][col]= weight\n",
    "            num_feature[row][3]=weight\n",
    "            \n",
    "    return adj_mat,num_feature\n",
    "\n",
    "def create_label_feature(entity):\n",
    "    entity_labels=['label-0', 'label-1', 'label-2', 'label-3', 'label-4', 'label-5', 'label-6', 'label-7', \n",
    "                   'label-8', 'label-9', 'label-10', 'label-11', \n",
    "                   'label-12', 'label-13', 'label-14', 'label-15', 'label-16', 'label-17', 'label-18', 'label-19']\n",
    "    \n",
    "    entity=entity.lower()\n",
    "    label=np.zeros((len(entity_labels)))\n",
    "    try:\n",
    "        label[entity_labels.index(entity)]=1\n",
    "    except ValueError:\n",
    "        label[19]=1\n",
    "    finally:\n",
    "        return label\n",
    "#--------------------------------model creation ---------------------------------------------------------   \n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, adj_mat_list, feature_vector_list, label_list=None):\n",
    "        self.adj_mat =adj_mat_list\n",
    "        self.feature=feature_vector_list\n",
    "        self.label= label_list\n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        A = csr_matrix(self.adj_mat[idx])\n",
    "        edge_index, edge_attr = convert.from_scipy_sparse_matrix(A)\n",
    "        x=torch.tensor(self.feature[idx], dtype=torch.float)\n",
    "        \n",
    "        if self.label != None:\n",
    "            y=torch.tensor(self.label[idx], dtype=torch.long)\n",
    "            data =Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "        else:\n",
    "            data =Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            \n",
    "        return data\n",
    "        \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,features_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = ChebConv(features_size, 16,3)\n",
    "        self.conv2 = ChebConv(16, 32,3)\n",
    "        self.conv3 = ChebConv(32, 64,3)\n",
    "        self.conv4 = ChebConv(64, 128,3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)    \n",
    "    \n",
    "def get_validation_loss(validloder, model, loss_fun, epoch,betch_size=2, train_on_gpu=False):\n",
    "     #validation loss     \n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "\n",
    "    # Don't need to keep track of gradients\n",
    "    with torch.no_grad():\n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "        # Validation loop\n",
    "        for data in validloder:\n",
    "            # Tensors to gpu\n",
    "            if train_on_gpu:\n",
    "                data= data.cuda()\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "#             print('output size {}:\\n{} '.format(output.size(),output))\n",
    "            # Validation loss\n",
    "            loss = loss_fun(output, data.y.long())\n",
    "#             pred = output.max(dim=1)[1]\n",
    "            # Multiply average loss times the number of examples in batch\n",
    "            valid_loss += loss.item() * betch_size\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            correct_tensor = pred.eq(data.y.view_as(pred))\n",
    "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "            # Multiply average accuracy times the number of examples\n",
    "            valid_acc += accuracy.item() * betch_size\n",
    "\n",
    "        # Calculate average losses\n",
    "        valid_loss = valid_loss / len(validloder.dataset)\n",
    "\n",
    "        # Calculate average accuracy\n",
    "        valid_acc = valid_acc / len(validloder.dataset)\n",
    "        \n",
    "#         print(f'\\nEpoch: {epoch} \\tValidation Loss: {valid_loss:.4f} \\tValidation Accuracy: {100 * valid_acc:.2f}')\n",
    "        \n",
    "    return valid_loss, valid_acc\n",
    "\n",
    "def get_taining_loss(trainloader,model, optimizer,loss_fun,epoch,betch_size=2,train_on_gpu=False):\n",
    "    # keep track of training and validation loss each epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0\n",
    "    \n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # trainig loop \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # Tensors to gpu\n",
    "        if train_on_gpu:\n",
    "            print('running on gpu')\n",
    "            data = data.cuda()\n",
    "            \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = model(data)\n",
    "#         print(type(output))\n",
    "        loss = loss_fun(output, data.y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track train loss by multiplying average loss by number of examples in batch\n",
    "        train_loss += loss.item() * betch_size\n",
    "\n",
    "        # Calculate accuracy by finding max log probability\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        correct_tensor = pred.eq(data.y.view_as(pred))\n",
    "        # Need to convert correct tensor from int to float to average\n",
    "        accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "       \n",
    "        # Multiply average accuracy times the number of examples in batch\n",
    "        train_acc += accuracy.item() * betch_size\n",
    "        \n",
    "        # Calculate average losses\n",
    "        train_loss = train_loss / len(trainloader)\n",
    "       \n",
    "        # Print training and validation results\n",
    "                \n",
    "#         print(f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tTraining Accuracy: {100 * train_acc:.2f}')\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def train_model(model,model_path,trainloder,validloder,No_epochs=2000,max_epochs_stop=50,print_every=100):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    Entropy_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Early stopping intialization\n",
    "    epochs_no_improve = 0\n",
    "    valid_loss_min = np.Inf\n",
    "    history = []\n",
    "    overall_start=timer()\n",
    "    for epoch in range(No_epochs):\n",
    "        start=timer()\n",
    "        train_loss, train_acc = get_taining_loss(trainloder,model,optimizer,Entropy_loss,epoch)\n",
    "        \n",
    "        valid_loss, valid_acc = get_validation_loss(validloder,model,Entropy_loss,epoch)\n",
    "        \n",
    "        history.append([train_loss, valid_loss, train_acc, valid_acc])  # adding data for plot\n",
    "        \n",
    "        # Print training and validation results\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\t\\tValidation Loss: {valid_loss:.4f}')\n",
    "#             print(f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\tValidation Accuracy: {100 * valid_acc:.2f}%')\n",
    "            print(f'\\t\\t\\t( {timer() - start:.2f} seconds elapsed in epoch {epoch})',end='\\r')\n",
    "        \n",
    "        # Save the model if validation loss decreases\n",
    "        if valid_loss < valid_loss_min:\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            # Track improvement\n",
    "            epochs_no_improve = 0\n",
    "            valid_loss_min = valid_loss\n",
    "#            valid_best_acc = valid_acc\n",
    "            best_epoch = epoch\n",
    "        # Otherwise increment count of epochs with no improvement\n",
    "        else:\n",
    "            epochs_no_improve += 1    \n",
    "            \n",
    "        # Trigger early stopping\n",
    "        if epochs_no_improve >= max_epochs_stop or epoch+1 >= No_epochs:\n",
    "            if( epoch+1 >= No_epochs):\n",
    "                print(f'\\n-----------------------------------Traning completed----------------------------------')\n",
    "                print(f'\\nCompleted Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%')\n",
    "            else:\n",
    "                print(f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%')\n",
    "                \n",
    "            total_time = timer() - overall_start\n",
    "            print(f'\\t({total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch)')\n",
    "\n",
    "            # Load the best state dict\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            # Attach the optimizer\n",
    "            model.optimizer = optimizer\n",
    "\n",
    "            # Format history\n",
    "            history = pd.DataFrame(history,columns=['train_loss', 'valid_loss', 'train_acc','valid_acc'])\n",
    "            return model, history\n",
    "\n",
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for c in ['train_loss', 'valid_loss']:\n",
    "        plt.plot(history[c], label=c)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Negative Log Likelihood')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    \n",
    "def plot_accuracy(history):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for c in ['train_acc', 'valid_acc']:\n",
    "        plt.plot(100 * history[c], label=c)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "def image_feature_processing(img_data=None,folder_path=None):\n",
    "    if img_data==None:\n",
    "        img_data,file_name = get_image_data(folder_path,True)\n",
    "    \n",
    "    adj_mat_list=[]\n",
    "    feature_vector_list=[]\n",
    "    word_info_list=[]\n",
    "    for idx,img in enumerate(img_data): \n",
    "        if folder_path !=None:\n",
    "            print('processing file (id {}) : {}'.format(idx,file_name[idx]))\n",
    "            \n",
    "        print('processing file id : {}'.format(idx))\n",
    "        word_bbox_list, bbox_width =  extract_bbox_info(img)\n",
    "        word_list = remove_noise(bbox_width,word_bbox_list)\n",
    "        lines = Line_Formation(word_list)\n",
    "        word_dict , contain_dict= get_word_dict(lines)\n",
    "        word_info_list.append([word_list,word_dict,contain_dict])\n",
    "        width, hight ,_ = img.shape\n",
    "        lookup= get_NN_lookup_table(lines,width,hight)\n",
    "        adj_mat, num_feature = create_graph_numeric_feature(word_dict,lookup)\n",
    "        feature_vector = create_feature_vector(word_dict,num_feature)\n",
    "        adj_mat_list.append(adj_mat)\n",
    "        feature_vector_list.append(feature_vector)\n",
    "        \n",
    "    return adj_mat_list,feature_vector_list,word_info_list\n",
    "\n",
    "def print_ocr_string(img):  \n",
    "    res = isinstance(img, str)\n",
    "    if res:\n",
    "        print('reading image ..\\n' )\n",
    "        img = cv2.imread(img)  \n",
    "    text = pytesseract.image_to_string(img)\n",
    "#     print(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------main ---------------------------------------------\n",
    "    \n",
    "    \n",
    "pincode_path=\"C:/Users/Sanjeev/Documents/Python Scripts/cheat Sheet/Pincode_30052019.csv\"\n",
    "# pincode_path= \"C:/Users/Sanjeev/Documents/Accenture office/Pincode_30052019.csv\"\n",
    "# country_path=\"C:/Users/Sanjeev/Documents/Accenture office/countries.csv\"\n",
    "country_path=\"C:/Users/Sanjeev/Documents/Accenture office/countries.txt\"\n",
    "database_path =\"C:/Users/Sanjeev/Documents/Accenture office/database\"\n",
    "img_path=\"C:/Users/Sanjeev/Documents/Accenture office/invoice3.png\"\n",
    "tesseract_path ='C:/python_lib/ocr/tesseract.exe'\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/python_lib/ocr/tesseract.exe'\n",
    "model_path = \"C:/Users/Sanjeev/Documents/Accenture office/invoice_model\"\n",
    "folder_path = \"C:/Users/Sanjeev/Documents/Accenture office/invoice data/*\"\n",
    "\n",
    "# folder_path = \"C:/Users/Sanjeev/Documents/Python Scripts/Dataset/output/*.png\"\n",
    "csv_path ='C:/Users/Sanjeev/Documents/Python Scripts/Dataset/csv/'\n",
    "data_path = \"C:/Users/Sanjeev/Documents/Python Scripts/Dataset/df_word_list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf file skiped\n",
      "pdf file skiped\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# reading the image file from the folder and stored as numpy array\n",
    "img_data, file_name = get_image_data(folder_path,True)\n",
    "print(len(img_data))\n",
    "#1,2,3,5  (issue in noise removal-> 4)\n",
    "# del img_data[4]\n",
    "# del file_name[4]\n",
    "print(len(img_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file id : 0\n",
      "processing file id : 1\n",
      "processing file id : 2\n"
     ]
    }
   ],
   "source": [
    "# pre-processing the image data \n",
    "# 1. feed the image to OCR and get HOCR file which contain bounding box infomation\n",
    "# 2. Create numeric, boolean, and word features \n",
    "# 3. Create Graph \n",
    "# 4. return adjacency metric A, feature vector 317 dim, and word infoamtion \n",
    "adj_mat_list,feature_vector_list, word_info_list = image_feature_processing(img_data=img_data[0:3])    # pre-processing and graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating label for the each node of the graph (dummy label for testing Neural Network pipeline)\n",
    "label_list=[]\n",
    "seed=1234567833\n",
    "np.random.seed(seed)\n",
    "for i in range(len(adj_mat_list)):\n",
    "    label = np.random.randint(0,10,adj_mat_list[i].shape[0])\n",
    "    label_list.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9 \tTraining Loss: 40.6291 \t\tValidation Loss: 43.2771\n",
      "\t\t\t( 0.10 seconds elapsed in epoch 9)\n",
      "-----------------------------------Traning completed----------------------------------\n",
      "\n",
      "Completed Total epochs: 9. Best epoch: 9 with loss: 43.28 and acc: 21.75%\n",
      "\t(1.37 total seconds elapsed. 0.14 seconds per epoch)\n"
     ]
    }
   ],
   "source": [
    "# creating dataset and dataloader for pytorch pipeline\n",
    "dataset =CreateDataset(adj_mat_list,feature_vector_list,label_list)\n",
    "\n",
    "betch_size=2\n",
    "train_dataloder = DataLoader(dataset,betch_size)\n",
    "torch.manual_seed(seed)\n",
    "model=Net(317)\n",
    "model , history = train_model(model,model_path,train_dataloder,train_dataloder,10,5,10) # train the Neural Network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting training and validation loss \n",
    "plot_loss(history)\n",
    "\n",
    "# ploting training and validation accuracy \n",
    "# plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing he model after training \n",
    "\n",
    "## yet to impement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of image, word detected by ocr, words after removal of noise, and Graph generated.\n",
    "\n",
    "idx=1\n",
    "img=cv2.imread(file_name[idx])\n",
    "display_image(img)\n",
    "\n",
    "word_bbox_list, bbox_width =  extract_bbox_info(img)\n",
    "display_bounding_box(img, word_bbox_list,'orginal words')\n",
    "\n",
    "# print_ocr_string(img)\n",
    "\n",
    "word_list = remove_noise(bbox_width,word_bbox_list)\n",
    "img=cv2.imread(file_name[idx])\n",
    "display_bounding_box(img, word_list,'word after removal')\n",
    "\n",
    "lines = Line_Formation(word_list)\n",
    "word_dict , contain_dict= get_word_dict(lines)\n",
    "width, hight ,_ = img.shape\n",
    "lookup= get_NN_lookup_table(lines,width,hight)\n",
    "adj_mat, num_feature = create_graph_numeric_feature(word_dict,lookup)\n",
    "\n",
    "img = cv2.imread(file_name[idx])\n",
    "display_connected_graph(img, lookup, 'merge',True)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "# feature_vector = create_feature_vector(word_dict,num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(file_name[idx])\n",
    "display_connected_graph(img, lookup, 'merge',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "No_feature=317\n",
    "def get_prediction(model,img_list=None,img_path=None):\n",
    "#     model.eval()\n",
    "    if img_list != None:\n",
    "        img_data= img_list\n",
    "    if img_path !=None:\n",
    "        img_data=img_path\n",
    "    adj_mat_list,feature_vector_list, word_info_list = image_feature_processing(img_data=img_data)\n",
    "    \n",
    "    betch_size = len(adj_mat_list)   # no of data for testing \n",
    "    \n",
    "    if betch_size==1:\n",
    "        print('betch_size : ',betch_size)\n",
    "        test_x=torch.tensor(feature_vector_list[0], dtype=torch.float) \n",
    "        test_edge_index, test_edge_attr = convert.from_scipy_sparse_matrix(csr_matrix(adj_mat_list[0]))\n",
    "        test_data=Data(x=test_x,edge_index=test_edge_index,edge_attr=test_edge_attr)\n",
    "    else:\n",
    "        print('betch_size : ',betch_size)\n",
    "        test_data =CreateDataset(adj_mat_list, feature_vector_list)\n",
    "        test_data = DataLoader(test_data,betch_size)\n",
    "        test_data = next(iter(test_data))\n",
    "\n",
    "    pred_out = model(test_data) \n",
    "    print(pred_out.size())\n",
    "    _ ,pred_out = torch.max(pred_out, dim=1)\n",
    "    \n",
    "    return pred_out\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.load_state_dict(torch.load(model_path))\n",
    "test_model.eval()\n",
    "# pred =get_prediction(test_model,img_data)\n",
    "# print(pred.size())\n",
    "# correct_tensor = pred.eq(test_y)\n",
    "# print(sum(correct_tensor))\n",
    "# print(correct_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_o=pred.numpy()\n",
    "p_o[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum=0\n",
    "for label in label_list:\n",
    "    print(len(label))\n",
    "    sum+= len(label)\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx=1\n",
    "\n",
    "test_x=torch.tensor(feature_vector_list[test_idx], dtype=torch.float)\n",
    "test_y=torch.tensor(label_list[test_idx], dtype=torch.long)\n",
    "# A = csr_matrix(adj_mat_list[test_idx])\n",
    "test_edge_index, test_edge_attr = convert.from_scipy_sparse_matrix(csr_matrix(adj_mat_list[test_idx]))\n",
    "test_data=Data(x=test_x,edge_index=test_edge_index,edge_attr=test_edge_attr)\n",
    "test_model = Net(317)\n",
    "test_model.load_state_dict(torch.load(model_path))\n",
    "test_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out= test_model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, pred = torch.max(pred_out, dim=1)\n",
    "correct_tensor = pred.eq(test_y)\n",
    "# correct_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text =f'sanjeev has good heart person and he is nice.His DOB is 15/05/1992.'\n",
    "# text =text.split(' ')[3]\n",
    "doc=nlp(text)\n",
    "# print(doc)\n",
    "\n",
    "# for token in doc:\n",
    "#     print('{} -> {}'.format(token.text, token.pos_))\n",
    "\n",
    "for token in doc.ents:\n",
    "    print('{} -> {}'.format(token.text, token.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7,
     29,
     40
    ]
   },
   "outputs": [],
   "source": [
    "#this function takes input word and out p\n",
    "# ORG -> [1,0,0,0]\n",
    "# Date -> [0,1,0,0]\n",
    "# GPE ->  [0,0,1,0]\n",
    "# MONEY/CARDINAL->[0,0,0,1]\n",
    "#except this -> [0,0,0,0]\n",
    "\n",
    "def get_ner_tag(ner_tag):\n",
    "    if len(ner_tag) is not 0:\n",
    "        ner_tag=ner_tag[0].label_\n",
    "        print(ner_tag)\n",
    "        if ner_tag=='ORG':\n",
    "            return [1,0,0,0]\n",
    "        elif ner_tag=='DATE':\n",
    "            return [0,1,0,0]\n",
    "        elif ner_tag=='GPE':\n",
    "            return [0,0,1,0]\n",
    "        elif ner_tag=='MONEY' or ner_tag=='CARDINAL':\n",
    "            return [0,0,0,1]\n",
    "        else:\n",
    "            return [0,0,0,0]\n",
    "    else:\n",
    "         return [0,0,0,0]\n",
    "    \n",
    "    \n",
    "\n",
    "#this function takes token/word as input and give 2-d output( 10: -> noun/pronoun,01-> Number, 00-> others )\n",
    "# [1,0,0]\n",
    "\n",
    "def get_pos_tag(pos_tags):\n",
    "    \n",
    "    if pos_tags =='NOUN' or pos_tags=='PRON':\n",
    "        return [1,0]\n",
    "    elif pos_tags =='NUM':\n",
    "        return [0,1]\n",
    "    else:\n",
    "        return [0,0]\n",
    "        \n",
    "    \n",
    "\n",
    "def create_tag_features(word_list):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    pos_tag_list = []\n",
    "    ner_tag_list=[]\n",
    "    for word in word_list:\n",
    "        tag =nlp(word)\n",
    "        pos_tag_list.append(get_pos_tag(tag[0].pos_))\n",
    "        ner_tag_list.append(get_ner_tag(tag.ents))\n",
    "    return pos_tag_list , ner_tag_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "pos, ner =create_tag_features(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos,ner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "img=cv2.imread(file_name[idx])\n",
    "# display_image(img)\n",
    "\n",
    "word_bbox_list, bbox_width =  extract_bbox_info(img)\n",
    "# word_bbox_list =noise_remove(word_bbox_list)\n",
    "display_bounding_box(img, word_bbox_list,'orginal words')\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def noise_remove(word_list,skip_symbol=None):\n",
    "    filter_list=[]\n",
    "    for w_list in word_list:\n",
    "        word= w_list[1]\n",
    "        if len(word)==1: # checking if word length is 1 it could be noise\n",
    "            if skip_symbol and word in skip_symbol:   # if word is in our desired symbol ex:- $,₹, etc \n",
    "                filter_list.append(w_list)\n",
    "            elif is_alphaNumeric(word):   # consider the case John D :- d has len one but it is alphabet.\n",
    "                filter_list.append(w_list)\n",
    "        else:\n",
    "            filter_list.append(w_list)\n",
    "    return filter_list    # this sshould be the main noise removal\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def row_clustring(lines_list, row_threshold=4):\n",
    "    row_bbox_list =[]\n",
    "    test_list=[]\n",
    "    merge_info=[] #  for storing the merge info\n",
    "    for idx,line in enumerate(lines_list):\n",
    "        temp_test_list=[]\n",
    "        curr_bbox_id=0\n",
    "        next_bbox_id=1\n",
    "        prev_id=-1\n",
    "        total_word=len(line) \n",
    "        print(line)\n",
    "        print('total word = ',total_word)\n",
    "        temp_merge=[]\n",
    "        info=[]\n",
    "#         temp_merge.append(line[curr_bbox_id])\n",
    "        curr_bbox=line[curr_bbox_id][2]\n",
    "        print('curr_bbox : ',curr_bbox)\n",
    "        \n",
    "        while next_bbox_id < total_word:\n",
    "            next_bbox=line[next_bbox_id][2]\n",
    "            print('next_bbox : ',line[next_bbox_id])\n",
    "            dist = np.sqrt((curr_bbox[2] - next_bbox[0])**2)    # calculating distance having y point common(mid point in overlaping case)\n",
    "#             dist = curr_bbox[2] - next_bbox[0] \n",
    "            print('dist ',dist)\n",
    "            if dist <= row_threshold:    # merging two bbox\n",
    "#                 print('min ----------------------dist-: ',dist)\n",
    "                curr_bbox[2]= max(curr_bbox[2],next_bbox[2])\n",
    "                curr_bbox[3]= max(curr_bbox[3],next_bbox[3])\n",
    "                print('curr_bbox (merging)',curr_bbox)\n",
    "                if prev_id != curr_bbox_id:\n",
    "                    info.append(line[curr_bbox_id])\n",
    "                    prev_id=curr_bbox_id\n",
    "                info.append(line[next_bbox_id])      \n",
    "            else:\n",
    "                row_bbox_list.append(curr_bbox)\n",
    "                print('row_bbox_list (inserting)',curr_bbox)\n",
    "                temp_test_list.append(curr_bbox)  # tesing \n",
    "                \n",
    "                temp_merge.append(info)\n",
    "                curr_bbox_id= next_bbox_id\n",
    "                curr_bbox=next_bbox\n",
    "                print('curr_bbox (adding)',curr_bbox)\n",
    "                info=[]\n",
    "            next_bbox_id +=1\n",
    "            print('next_bbox_id ', next_bbox_id)\n",
    "       \n",
    "        merge_info.append(temp_merge)\n",
    "        if dist > row_threshold:\n",
    "            row_bbox_list.append(line[next_bbox_id-1][2])\n",
    "            print('row_bbox_list (inserting) after last word',line[next_bbox_id-1])\n",
    "            temp_test_list.append(line[next_bbox_id-1][2])\n",
    "        else:\n",
    "            row_bbox_list.append(curr_bbox)\n",
    "            print('row_bbox_list (inserting) after merge',curr_bbox)\n",
    "            temp_test_list.append(curr_bbox)\n",
    "        test_list.append(temp_test_list)\n",
    "        print('---------------------------line {}-----------------------------'.format(idx+1))\n",
    "    return row_bbox_list,merge_info,test_list\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_overlaping(source,target):\n",
    "    source_BL = source[0]  # assume the co-ordinates as (y,x) and bounding box has (y,x,h,w) ->(left, top, right ,bottom)\n",
    "    source_BR = source[2]           \n",
    "    target_TL = target[0]\n",
    "    target_TR = target[2]\n",
    "    \n",
    "    \n",
    "    case1 =source_BL <= target_TR <= source_BR  # case 1. target right overlap with source in bottom (vertically)\n",
    "    case2 =source_BL <= target_TL<=source_BR   # case 2. target left overlap with source  in bottom\n",
    "    case3 =target_TL < source_BL < target_TR   #case 3 . source overlap under target  in bottom\n",
    "\n",
    "    if case1 or case2 or case3: \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# we always check from top row to next bottom row for merging two boxes.             \n",
    "def col_clestring(row_merge_list, col_threshold):\n",
    "    print('enter for calculaton ')\n",
    "    merge_list =[]\n",
    "    merge_info=[]\n",
    "    top_row= row_merge_list[0]\n",
    "    for idx in range(1,len(row_merge_list)):\n",
    "        print('loop ', idx)\n",
    "        bottom_row = row_merge_list[idx]\n",
    "        top_row_len, bottom_row_len = len(top_row), len(bottom_row)\n",
    "        i, j,  candidate_row=0,0,[]\n",
    "#         print('top_row ', top_row)\n",
    "#         print('bottom_row ', bottom_row)\n",
    "#         print('len of rows top ={} ,botttom {} , i= {}, j= {}'.format(top_row_len,bottom_row_len, i, j))\n",
    "        merge_flag=False\n",
    "        while i < top_row_len:\n",
    "            while j < bottom_row_len:\n",
    "                if (i>= top_row_len):\n",
    "                    print('breaking top row loop')\n",
    "                    break\n",
    "                top_bbox = top_row[i]\n",
    "                bottom_bbox= bottom_row[j]\n",
    "                \n",
    "                print('top_row ', top_row)\n",
    "                print('bottom_row ', bottom_row)\n",
    "                print('len of rows top ={} ,botttom {} , i= {}, j= {}'.format(top_row_len,bottom_row_len, i, j))\n",
    "                print('top_bbox = {} \\nbottom_bbox = {}'.format(top_bbox,bottom_bbox) )\n",
    "                if check_overlaping(top_bbox,bottom_bbox):    # when top_bbox and bottom_bbox overlap each other \n",
    "                    print('overlaping  ')\n",
    "                    dist = np.sqrt(( bottom_bbox[1] - top_bbox[3])**2)\n",
    "                    print('dist : ', dist)\n",
    "                    if dist <=col_threshold:\n",
    "                        print('distance statisfy : ',j)\n",
    "                        top_bbox[0]= min(top_bbox[0],bottom_bbox[0])\n",
    "                        top_bbox[1]= min(top_bbox[1],bottom_bbox[1])\n",
    "                        top_bbox[2]= max(top_bbox[2],bottom_bbox[2])\n",
    "                        top_bbox[3]= max(top_bbox[3],bottom_bbox[3])\n",
    "                        j +=1\n",
    "                        merge_flag=True\n",
    "                        \n",
    "                        if j >= bottom_row_len:  # handling last box box when it merge \n",
    "                            candidate_row.append(top_bbox)\n",
    "                            print('adding *last* candidate row (top box)  ', top_bbox)\n",
    "                        \n",
    "                    else:\n",
    "                        print('distance not statisfy: ', j)\n",
    "                        candidate_row.append(bottom_bbox)\n",
    "                        print('adding to candidate row (bottom_bbox) ', bottom_bbox)\n",
    "                        j +=1\n",
    "                else:\n",
    "                    print('Not overlaping  ')\n",
    "                    if(top_bbox[2] < bottom_bbox[0]):    # bottom_bbox away(right most) from top_bbox \n",
    "                        if merge_flag:\n",
    "                            candidate_row.append(top_bbox)\n",
    "                            print('(right most) adding to candidate row (top box) ', top_bbox)\n",
    "                        merge_list.append(top_bbox)\n",
    "                        print('(right most)adding to mergeList  box ', top_bbox)\n",
    "                        i +=1\n",
    "                    else:\n",
    "                        candidate_row.append(bottom_bbox)\n",
    "                        print('(left most) adding to candidate row (bottom_bbox) ', bottom_bbox)\n",
    "                        j += 1\n",
    "                    \n",
    "                print('value i= {} and j ={} '.format(i,j))\n",
    "                print('merge_list : ', merge_list)\n",
    "                print('candidate_row :',candidate_row)\n",
    "                print('\\n--exsiting the while 1 loop--\\n') \n",
    "            \n",
    "            while(j < bottom_row_len): # case when top row empty and bottom row has word \n",
    "                print('bottom bbox is not empty : ', j)\n",
    "                candidate_row.append(bottom_row[j])\n",
    "                j +=1\n",
    "                print('value i= {} and j ={} '.format(i,j))\n",
    "            while(i < top_row_len): # case when bottom row empty and top row has word \n",
    "                print('top box  is not empty : ', i)\n",
    "                merge_list.append(top_row[i])\n",
    "                i +=1\n",
    "                print('value i= {} and j ={} '.format(i,j))\n",
    "        print('exsiting the while 2 loop')\n",
    "        print('merge_list : ', merge_list)\n",
    "        print('candidate_row :',candidate_row)\n",
    "        top_row = candidate_row\n",
    "        print('candidate_row ', candidate_row)\n",
    "        print('\\n---------------------------------------------------------------\\n')\n",
    "#     merge_list.append(top_row)\n",
    "    return  merge_list+top_row\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = col_clestring(c, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization of image, word detected by ocr, words after removal of noise, and Graph generated.\n",
    "\n",
    "idx=1\n",
    "img=cv2.imread(file_name[idx])\n",
    "# display_image(img)\n",
    "\n",
    "word_bbox_list, bbox_width =  extract_bbox_info(img)\n",
    "# display_bounding_box(img, word_bbox_list,'orginal words')\n",
    "\n",
    "# print_ocr_string(img)\n",
    "\n",
    "word_list = remove_noise(bbox_width,word_bbox_list)\n",
    "img=cv2.imread(file_name[idx])\n",
    "# display_bounding_box(img, word_list,'word after removal')\n",
    "\n",
    "lines = Line_Formation(word_list)\n",
    "word_dict , contain_dict= get_word_dict(lines)\n",
    "width, hight ,_ = img.shape\n",
    "lookup= get_NN_lookup_table(lines,width,hight)\n",
    "adj_mat, num_feature = create_graph_numeric_feature(word_dict,lookup)\n",
    "\n",
    "img = cv2.imread(file_name[idx])\n",
    "# display_connected_graph(img, lookup, 'merge',True)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "# feature_vector = create_feature_vector(word_dict,num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['word_1_4', '\"Romashka\"', [49, 44, 120, 54]],\n",
       "  ['word_1_5', 'Ltd.', [125, 44, 145, 54]],\n",
       "  ['word_1_1', 'Invoice', [411, 34, 460, 45]],\n",
       "  ['word_1_2', 'ID:', [465, 34, 483, 45]],\n",
       "  ['word_1_3', 'INV/20111209-22', [567, 34, 679, 45]]],\n",
       " [['word_1_6', '1600', [49, 60, 77, 70]],\n",
       "  ['word_1_7', 'Amphitheatre', [81, 60, 159, 73]],\n",
       "  ['word_1_8', 'Parkway', [164, 60, 213, 73]],\n",
       "  ['word_1_9', 'Mountain', [217, 60, 270, 70]],\n",
       "  ['word_1_10', 'View,', [275, 60, 306, 72]],\n",
       "  ['word_1_11', 'CA', [311, 60, 328, 70]],\n",
       "  ['word_1_12', '94043', [332, 60, 368, 70]],\n",
       "  ['word_1_13', 'Invoice', [411, 59, 460, 70]],\n",
       "  ['word_1_14', 'date:', [465, 59, 498, 70]],\n",
       "  ['word_1_15', '12/08/2011', [567, 58, 635, 69]]],\n",
       " [['word_1_16', 'Due', [411, 83, 438, 94]],\n",
       "  ['word_1_17', 'date:', [442, 83, 476, 94]],\n",
       "  ['word_1_18', '12/25/2012', [567, 83, 638, 94]]],\n",
       " [['word_1_19', 'Text', [411, 108, 440, 119]],\n",
       "  ['word_1_20', 'custom', [444, 109, 495, 119]],\n",
       "  ['word_1_21', 'fiel', [500, 108, 520, 119]],\n",
       "  ['word_1_22', 'Visible', [566, 107, 608, 118]],\n",
       "  ['word_1_23', 'field', [612, 107, 638, 118]],\n",
       "  ['word_1_24', 'in', [643, 107, 653, 118]],\n",
       "  ['word_1_25', 'PDF', [659, 107, 687, 118]]],\n",
       " [['word_1_28', '#', [48, 244, 56, 253]],\n",
       "  ['word_1_29', 'Description', [76, 243, 147, 256]],\n",
       "  ['word_1_30', 'Qty', [417, 243, 438, 256]],\n",
       "  ['word_1_31', 'Units', [467, 243, 498, 253]],\n",
       "  ['word_1_32', 'Unit', [520, 234, 545, 263]],\n",
       "  ['word_1_33', 'price', [550, 234, 583, 263]],\n",
       "  ['word_1_34', '(EUR)', [587, 243, 620, 256]],\n",
       "  ['word_1_35', 'Total', [637, 234, 668, 263]],\n",
       "  ['word_1_36', '(EUR)', [674, 243, 709, 256]]],\n",
       " [['word_1_38', 'Projecting', [76, 272, 133, 285]],\n",
       "  ['word_1_39', 'x10', [415, 272, 439, 282]],\n",
       "  ['word_1_40', 'hours', [464, 272, 496, 282]],\n",
       "  ['word_1_41', '50.00', [587, 272, 619, 282]],\n",
       "  ['word_1_42', '50.00', [676, 272, 708, 282]]],\n",
       " [['word_1_44', 'Context', [83, 288, 128, 298]],\n",
       "  ['word_1_45', 'menu', [133, 291, 164, 298]],\n",
       "  ['word_1_46', 'for', [169, 288, 184, 298]],\n",
       "  ['word_1_47', 'invoices', [188, 288, 235, 298]],\n",
       "  ['word_1_48', 'list', [239, 288, 255, 298]]],\n",
       " [['word_1_49', '2', [48, 318, 55, 328]],\n",
       "  ['word_1_50', 'Develop', [76, 318, 123, 331]],\n",
       "  ['word_1_51', 'x17.0', [412, 318, 443, 328]],\n",
       "  ['word_1_52', 'hours', [464, 318, 496, 328]],\n",
       "  ['word_1_53', '40.00', [587, 318, 619, 328]],\n",
       "  ['word_1_54', '680.00', [669, 318, 708, 328]]],\n",
       " [['word_1_56', 'Invoice', [84, 333, 124, 343]],\n",
       "  ['word_1_57', 'number', [129, 333, 173, 343]],\n",
       "  ['word_1_58', 'format', [177, 333, 214, 343]],\n",
       "  ['word_1_59', 'template', [217, 333, 268, 346]]],\n",
       " [['word_1_61', '[PRO]', [84, 349, 118, 362]],\n",
       "  ['word_1_62', 'Duplicating', [124, 349, 187, 362]],\n",
       "  ['word_1_63', 'invoices', [192, 352, 238, 359]]],\n",
       " [['word_1_65', 'Language', [84, 365, 141, 378]],\n",
       "  ['word_1_66', 'support', [146, 366, 189, 378]]],\n",
       " [['word_1_68', 'Context', [83, 380, 128, 390]],\n",
       "  ['word_1_69', 'menu', [133, 383, 164, 390]],\n",
       "  ['word_1_70', 'for', [169, 380, 184, 390]],\n",
       "  ['word_1_71', 'invoices', [188, 380, 235, 390]],\n",
       "  ['word_1_72', 'list', [239, 380, 255, 390]]],\n",
       " [['word_1_73', '3', [48, 410, 55, 420]],\n",
       "  ['word_1_74', 'Analysi', [75, 410, 114, 423]],\n",
       "  ['word_1_75', '3.0', [415, 410, 439, 420]],\n",
       "  ['word_1_76', 'hours', [464, 410, 496, 420]],\n",
       "  ['word_1_77', '35.00', [587, 410, 619, 420]],\n",
       "  ['word_1_78', '105.00', [670, 410, 708, 420]]],\n",
       " [['word_1_80', '[PRO]', [84, 426, 118, 439]],\n",
       "  ['word_1_81', 'Duplicating', [124, 426, 187, 439]],\n",
       "  ['word_1_82', 'invoices', [192, 429, 238, 436]]],\n",
       " [['word_1_84', 'Language', [84, 442, 141, 455]],\n",
       "  ['word_1_85', 'support', [146, 443, 189, 455]]],\n",
       " [['word_1_90', '835.00', [665, 488, 709, 499]]],\n",
       " [['word_1_91', 'Tax', [411, 513, 436, 524]],\n",
       "  ['word_1_92', '(18.0%):', [440, 513, 494, 527]],\n",
       "  ['word_1_93', '150.30', [666, 513, 709, 524]]],\n",
       " [['word_1_94', 'Discount', [412, 537, 472, 548]],\n",
       "  ['word_1_95', '(10.0%):', [477, 537, 531, 551]],\n",
       "  ['word_1_96', '-83.50', [668, 537, 709, 548]]],\n",
       " [['word_1_97', 'Total', [410, 562, 445, 573]],\n",
       "  ['word_1_98', '(EUR):', [450, 562, 494, 576]],\n",
       "  ['word_1_99', '901.80', [665, 562, 709, 573]]],\n",
       " [['word_1_100', 'Your', [42, 612, 69, 622]],\n",
       "  ['word_1_101', 'billing', [74, 612, 106, 625]],\n",
       "  ['word_1_102', 'information', [110, 612, 174, 622]],\n",
       "  ['word_1_103', '(Bank,', [179, 612, 216, 625]],\n",
       "  ['word_1_104', 'Address,', [220, 612, 272, 624]],\n",
       "  ['word_1_105', 'IBAN,', [277, 612, 310, 624]],\n",
       "  ['word_1_106', 'SWIFT', [314, 612, 355, 622]],\n",
       "  ['word_1_107', '&', [359, 612, 367, 622]],\n",
       "  ['word_1_108', 'etc.)', [371, 612, 396, 625]]],\n",
       " [['word_1_109', 'Was', [42, 641, 68, 651]],\n",
       "  ['word_1_112', 'Beschreibung', [110, 641, 189, 654]],\n",
       "  ['word_1_110', 'ist', [72, 642, 85, 651]],\n",
       "  ['word_1_111', 'mit', [89, 642, 106, 651]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['word_1_4', '\"Romashka\"', [49, 44, 145, 54]],\n",
       "  ['word_1_5', 'Ltd.', [125, 44, 145, 54]],\n",
       "  ['word_1_1', 'Invoice', [411, 34, 483, 45]],\n",
       "  ['word_1_2', 'ID:', [465, 34, 483, 45]],\n",
       "  ['word_1_3', 'INV/20111209-22', [567, 34, 679, 45]]],\n",
       " [['word_1_6', '1600', [49, 60, 368, 73]],\n",
       "  ['word_1_7', 'Amphitheatre', [81, 60, 159, 73]],\n",
       "  ['word_1_8', 'Parkway', [164, 60, 213, 73]],\n",
       "  ['word_1_9', 'Mountain', [217, 60, 270, 70]],\n",
       "  ['word_1_10', 'View,', [275, 60, 306, 72]],\n",
       "  ['word_1_11', 'CA', [311, 60, 328, 70]],\n",
       "  ['word_1_12', '94043', [332, 60, 368, 70]],\n",
       "  ['word_1_13', 'Invoice', [411, 59, 498, 70]],\n",
       "  ['word_1_14', 'date:', [465, 59, 498, 70]],\n",
       "  ['word_1_15', '12/08/2011', [567, 58, 635, 69]]],\n",
       " [['word_1_16', 'Due', [411, 83, 476, 94]],\n",
       "  ['word_1_17', 'date:', [442, 83, 476, 94]],\n",
       "  ['word_1_18', '12/25/2012', [567, 83, 638, 94]]],\n",
       " [['word_1_19', 'Text', [411, 108, 520, 119]],\n",
       "  ['word_1_20', 'custom', [444, 109, 495, 119]],\n",
       "  ['word_1_21', 'fiel', [500, 108, 520, 119]],\n",
       "  ['word_1_22', 'Visible', [566, 107, 687, 118]],\n",
       "  ['word_1_23', 'field', [612, 107, 638, 118]],\n",
       "  ['word_1_24', 'in', [643, 107, 653, 118]],\n",
       "  ['word_1_25', 'PDF', [659, 107, 687, 118]]],\n",
       " [['word_1_28', '#', [48, 244, 56, 253]],\n",
       "  ['word_1_29', 'Description', [76, 243, 147, 256]],\n",
       "  ['word_1_30', 'Qty', [417, 243, 438, 256]],\n",
       "  ['word_1_31', 'Units', [467, 243, 498, 253]],\n",
       "  ['word_1_32', 'Unit', [520, 234, 620, 263]],\n",
       "  ['word_1_33', 'price', [550, 234, 583, 263]],\n",
       "  ['word_1_34', '(EUR)', [587, 243, 620, 256]],\n",
       "  ['word_1_35', 'Total', [637, 234, 709, 263]],\n",
       "  ['word_1_36', '(EUR)', [674, 243, 709, 256]]],\n",
       " [['word_1_38', 'Projecting', [76, 272, 133, 285]],\n",
       "  ['word_1_39', 'x10', [415, 272, 439, 282]],\n",
       "  ['word_1_40', 'hours', [464, 272, 496, 282]],\n",
       "  ['word_1_41', '50.00', [587, 272, 619, 282]],\n",
       "  ['word_1_42', '50.00', [676, 272, 708, 282]]],\n",
       " [['word_1_44', 'Context', [83, 288, 255, 298]],\n",
       "  ['word_1_45', 'menu', [133, 291, 164, 298]],\n",
       "  ['word_1_46', 'for', [169, 288, 184, 298]],\n",
       "  ['word_1_47', 'invoices', [188, 288, 235, 298]],\n",
       "  ['word_1_48', 'list', [239, 288, 255, 298]]],\n",
       " [['word_1_49', '2', [48, 318, 55, 328]],\n",
       "  ['word_1_50', 'Develop', [76, 318, 123, 331]],\n",
       "  ['word_1_51', 'x17.0', [412, 318, 443, 328]],\n",
       "  ['word_1_52', 'hours', [464, 318, 496, 328]],\n",
       "  ['word_1_53', '40.00', [587, 318, 619, 328]],\n",
       "  ['word_1_54', '680.00', [669, 318, 708, 328]]],\n",
       " [['word_1_56', 'Invoice', [84, 333, 268, 346]],\n",
       "  ['word_1_57', 'number', [129, 333, 173, 343]],\n",
       "  ['word_1_58', 'format', [177, 333, 214, 343]],\n",
       "  ['word_1_59', 'template', [217, 333, 268, 346]]],\n",
       " [['word_1_61', '[PRO]', [84, 349, 238, 362]],\n",
       "  ['word_1_62', 'Duplicating', [124, 349, 187, 362]],\n",
       "  ['word_1_63', 'invoices', [192, 352, 238, 359]]],\n",
       " [['word_1_65', 'Language', [84, 365, 189, 378]],\n",
       "  ['word_1_66', 'support', [146, 366, 189, 378]]],\n",
       " [['word_1_68', 'Context', [83, 380, 255, 390]],\n",
       "  ['word_1_69', 'menu', [133, 383, 164, 390]],\n",
       "  ['word_1_70', 'for', [169, 380, 184, 390]],\n",
       "  ['word_1_71', 'invoices', [188, 380, 235, 390]],\n",
       "  ['word_1_72', 'list', [239, 380, 255, 390]]],\n",
       " [['word_1_73', '3', [48, 410, 55, 420]],\n",
       "  ['word_1_74', 'Analysi', [75, 410, 114, 423]],\n",
       "  ['word_1_75', '3.0', [415, 410, 439, 420]],\n",
       "  ['word_1_76', 'hours', [464, 410, 496, 420]],\n",
       "  ['word_1_77', '35.00', [587, 410, 619, 420]],\n",
       "  ['word_1_78', '105.00', [670, 410, 708, 420]]],\n",
       " [['word_1_80', '[PRO]', [84, 426, 238, 439]],\n",
       "  ['word_1_81', 'Duplicating', [124, 426, 187, 439]],\n",
       "  ['word_1_82', 'invoices', [192, 429, 238, 436]]],\n",
       " [['word_1_84', 'Language', [84, 442, 189, 455]],\n",
       "  ['word_1_85', 'support', [146, 443, 189, 455]]],\n",
       " [['word_1_90', '835.00', [665, 488, 709, 499]]],\n",
       " [['word_1_91', 'Tax', [411, 513, 494, 527]],\n",
       "  ['word_1_92', '(18.0%):', [440, 513, 494, 527]],\n",
       "  ['word_1_93', '150.30', [666, 513, 709, 524]]],\n",
       " [['word_1_94', 'Discount', [412, 537, 531, 551]],\n",
       "  ['word_1_95', '(10.0%):', [477, 537, 531, 551]],\n",
       "  ['word_1_96', '-83.50', [668, 537, 709, 548]]],\n",
       " [['word_1_97', 'Total', [410, 562, 494, 576]],\n",
       "  ['word_1_98', '(EUR):', [450, 562, 494, 576]],\n",
       "  ['word_1_99', '901.80', [665, 562, 709, 573]]],\n",
       " [['word_1_100', 'Your', [42, 612, 396, 625]],\n",
       "  ['word_1_101', 'billing', [74, 612, 106, 625]],\n",
       "  ['word_1_102', 'information', [110, 612, 174, 622]],\n",
       "  ['word_1_103', '(Bank,', [179, 612, 216, 625]],\n",
       "  ['word_1_104', 'Address,', [220, 612, 272, 624]],\n",
       "  ['word_1_105', 'IBAN,', [277, 612, 310, 624]],\n",
       "  ['word_1_106', 'SWIFT', [314, 612, 355, 622]],\n",
       "  ['word_1_107', '&', [359, 612, 367, 622]],\n",
       "  ['word_1_108', 'etc.)', [371, 612, 396, 625]]],\n",
       " [['word_1_109', 'Was', [42, 641, 68, 651]],\n",
       "  ['word_1_112', 'Beschreibung', [110, 641, 189, 654]],\n",
       "  ['word_1_110', 'ist', [72, 642, 106, 651]],\n",
       "  ['word_1_111', 'mit', [89, 642, 106, 651]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['word_1_4', '\"Romashka\"', [49, 44, 120, 54]], ['word_1_5', 'Ltd.', [125, 44, 145, 54]], ['word_1_1', 'Invoice', [411, 34, 460, 45]], ['word_1_2', 'ID:', [465, 34, 483, 45]], ['word_1_3', 'INV/20111209-22', [567, 34, 679, 45]]]\n",
      "total word =  5\n",
      "curr_bbox :  [49, 44, 120, 54]\n",
      "next_bbox :  ['word_1_5', 'Ltd.', [125, 44, 145, 54]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [49, 44, 145, 54]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_1', 'Invoice', [411, 34, 460, 45]]\n",
      "dist  266.0\n",
      "row_bbox_list (inserting) [49, 44, 145, 54]\n",
      "curr_bbox (adding) [411, 34, 460, 45]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_2', 'ID:', [465, 34, 483, 45]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [411, 34, 483, 45]\n",
      "next_bbox_id  4\n",
      "next_bbox :  ['word_1_3', 'INV/20111209-22', [567, 34, 679, 45]]\n",
      "dist  84.0\n",
      "row_bbox_list (inserting) [411, 34, 483, 45]\n",
      "curr_bbox (adding) [567, 34, 679, 45]\n",
      "next_bbox_id  5\n",
      "row_bbox_list (inserting) after last word ['word_1_3', 'INV/20111209-22', [567, 34, 679, 45]]\n",
      "---------------------------line 1-----------------------------\n",
      "[['word_1_6', '1600', [49, 60, 77, 70]], ['word_1_7', 'Amphitheatre', [81, 60, 159, 73]], ['word_1_8', 'Parkway', [164, 60, 213, 73]], ['word_1_9', 'Mountain', [217, 60, 270, 70]], ['word_1_10', 'View,', [275, 60, 306, 72]], ['word_1_11', 'CA', [311, 60, 328, 70]], ['word_1_12', '94043', [332, 60, 368, 70]], ['word_1_13', 'Invoice', [411, 59, 460, 70]], ['word_1_14', 'date:', [465, 59, 498, 70]], ['word_1_15', '12/08/2011', [567, 58, 635, 69]]]\n",
      "total word =  10\n",
      "curr_bbox :  [49, 60, 77, 70]\n",
      "next_bbox :  ['word_1_7', 'Amphitheatre', [81, 60, 159, 73]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [49, 60, 159, 73]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_8', 'Parkway', [164, 60, 213, 73]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [49, 60, 213, 73]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_9', 'Mountain', [217, 60, 270, 70]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [49, 60, 270, 73]\n",
      "next_bbox_id  4\n",
      "next_bbox :  ['word_1_10', 'View,', [275, 60, 306, 72]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [49, 60, 306, 73]\n",
      "next_bbox_id  5\n",
      "next_bbox :  ['word_1_11', 'CA', [311, 60, 328, 70]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [49, 60, 328, 73]\n",
      "next_bbox_id  6\n",
      "next_bbox :  ['word_1_12', '94043', [332, 60, 368, 70]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [49, 60, 368, 73]\n",
      "next_bbox_id  7\n",
      "next_bbox :  ['word_1_13', 'Invoice', [411, 59, 460, 70]]\n",
      "dist  43.0\n",
      "row_bbox_list (inserting) [49, 60, 368, 73]\n",
      "curr_bbox (adding) [411, 59, 460, 70]\n",
      "next_bbox_id  8\n",
      "next_bbox :  ['word_1_14', 'date:', [465, 59, 498, 70]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [411, 59, 498, 70]\n",
      "next_bbox_id  9\n",
      "next_bbox :  ['word_1_15', '12/08/2011', [567, 58, 635, 69]]\n",
      "dist  69.0\n",
      "row_bbox_list (inserting) [411, 59, 498, 70]\n",
      "curr_bbox (adding) [567, 58, 635, 69]\n",
      "next_bbox_id  10\n",
      "row_bbox_list (inserting) after last word ['word_1_15', '12/08/2011', [567, 58, 635, 69]]\n",
      "---------------------------line 2-----------------------------\n",
      "[['word_1_16', 'Due', [411, 83, 438, 94]], ['word_1_17', 'date:', [442, 83, 476, 94]], ['word_1_18', '12/25/2012', [567, 83, 638, 94]]]\n",
      "total word =  3\n",
      "curr_bbox :  [411, 83, 438, 94]\n",
      "next_bbox :  ['word_1_17', 'date:', [442, 83, 476, 94]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [411, 83, 476, 94]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_18', '12/25/2012', [567, 83, 638, 94]]\n",
      "dist  91.0\n",
      "row_bbox_list (inserting) [411, 83, 476, 94]\n",
      "curr_bbox (adding) [567, 83, 638, 94]\n",
      "next_bbox_id  3\n",
      "row_bbox_list (inserting) after last word ['word_1_18', '12/25/2012', [567, 83, 638, 94]]\n",
      "---------------------------line 3-----------------------------\n",
      "[['word_1_19', 'Text', [411, 108, 440, 119]], ['word_1_20', 'custom', [444, 109, 495, 119]], ['word_1_21', 'fiel', [500, 108, 520, 119]], ['word_1_22', 'Visible', [566, 107, 608, 118]], ['word_1_23', 'field', [612, 107, 638, 118]], ['word_1_24', 'in', [643, 107, 653, 118]], ['word_1_25', 'PDF', [659, 107, 687, 118]]]\n",
      "total word =  7\n",
      "curr_bbox :  [411, 108, 440, 119]\n",
      "next_bbox :  ['word_1_20', 'custom', [444, 109, 495, 119]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [411, 108, 495, 119]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_21', 'fiel', [500, 108, 520, 119]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [411, 108, 520, 119]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_22', 'Visible', [566, 107, 608, 118]]\n",
      "dist  46.0\n",
      "row_bbox_list (inserting) [411, 108, 520, 119]\n",
      "curr_bbox (adding) [566, 107, 608, 118]\n",
      "next_bbox_id  4\n",
      "next_bbox :  ['word_1_23', 'field', [612, 107, 638, 118]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [566, 107, 638, 118]\n",
      "next_bbox_id  5\n",
      "next_bbox :  ['word_1_24', 'in', [643, 107, 653, 118]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [566, 107, 653, 118]\n",
      "next_bbox_id  6\n",
      "next_bbox :  ['word_1_25', 'PDF', [659, 107, 687, 118]]\n",
      "dist  6.0\n",
      "curr_bbox (merging) [566, 107, 687, 118]\n",
      "next_bbox_id  7\n",
      "row_bbox_list (inserting) after merge [566, 107, 687, 118]\n",
      "---------------------------line 4-----------------------------\n",
      "[['word_1_28', '#', [48, 244, 56, 253]], ['word_1_29', 'Description', [76, 243, 147, 256]], ['word_1_30', 'Qty', [417, 243, 438, 256]], ['word_1_31', 'Units', [467, 243, 498, 253]], ['word_1_32', 'Unit', [520, 234, 545, 263]], ['word_1_33', 'price', [550, 234, 583, 263]], ['word_1_34', '(EUR)', [587, 243, 620, 256]], ['word_1_35', 'Total', [637, 234, 668, 263]], ['word_1_36', '(EUR)', [674, 243, 709, 256]]]\n",
      "total word =  9\n",
      "curr_bbox :  [48, 244, 56, 253]\n",
      "next_bbox :  ['word_1_29', 'Description', [76, 243, 147, 256]]\n",
      "dist  20.0\n",
      "row_bbox_list (inserting) [48, 244, 56, 253]\n",
      "curr_bbox (adding) [76, 243, 147, 256]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_30', 'Qty', [417, 243, 438, 256]]\n",
      "dist  270.0\n",
      "row_bbox_list (inserting) [76, 243, 147, 256]\n",
      "curr_bbox (adding) [417, 243, 438, 256]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_31', 'Units', [467, 243, 498, 253]]\n",
      "dist  29.0\n",
      "row_bbox_list (inserting) [417, 243, 438, 256]\n",
      "curr_bbox (adding) [467, 243, 498, 253]\n",
      "next_bbox_id  4\n",
      "next_bbox :  ['word_1_32', 'Unit', [520, 234, 545, 263]]\n",
      "dist  22.0\n",
      "row_bbox_list (inserting) [467, 243, 498, 253]\n",
      "curr_bbox (adding) [520, 234, 545, 263]\n",
      "next_bbox_id  5\n",
      "next_bbox :  ['word_1_33', 'price', [550, 234, 583, 263]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [520, 234, 583, 263]\n",
      "next_bbox_id  6\n",
      "next_bbox :  ['word_1_34', '(EUR)', [587, 243, 620, 256]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [520, 234, 620, 263]\n",
      "next_bbox_id  7\n",
      "next_bbox :  ['word_1_35', 'Total', [637, 234, 668, 263]]\n",
      "dist  17.0\n",
      "row_bbox_list (inserting) [520, 234, 620, 263]\n",
      "curr_bbox (adding) [637, 234, 668, 263]\n",
      "next_bbox_id  8\n",
      "next_bbox :  ['word_1_36', '(EUR)', [674, 243, 709, 256]]\n",
      "dist  6.0\n",
      "curr_bbox (merging) [637, 234, 709, 263]\n",
      "next_bbox_id  9\n",
      "row_bbox_list (inserting) after merge [637, 234, 709, 263]\n",
      "---------------------------line 5-----------------------------\n",
      "[['word_1_38', 'Projecting', [76, 272, 133, 285]], ['word_1_39', 'x10', [415, 272, 439, 282]], ['word_1_40', 'hours', [464, 272, 496, 282]], ['word_1_41', '50.00', [587, 272, 619, 282]], ['word_1_42', '50.00', [676, 272, 708, 282]]]\n",
      "total word =  5\n",
      "curr_bbox :  [76, 272, 133, 285]\n",
      "next_bbox :  ['word_1_39', 'x10', [415, 272, 439, 282]]\n",
      "dist  282.0\n",
      "row_bbox_list (inserting) [76, 272, 133, 285]\n",
      "curr_bbox (adding) [415, 272, 439, 282]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_40', 'hours', [464, 272, 496, 282]]\n",
      "dist  25.0\n",
      "row_bbox_list (inserting) [415, 272, 439, 282]\n",
      "curr_bbox (adding) [464, 272, 496, 282]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_41', '50.00', [587, 272, 619, 282]]\n",
      "dist  91.0\n",
      "row_bbox_list (inserting) [464, 272, 496, 282]\n",
      "curr_bbox (adding) [587, 272, 619, 282]\n",
      "next_bbox_id  4\n",
      "next_bbox :  ['word_1_42', '50.00', [676, 272, 708, 282]]\n",
      "dist  57.0\n",
      "row_bbox_list (inserting) [587, 272, 619, 282]\n",
      "curr_bbox (adding) [676, 272, 708, 282]\n",
      "next_bbox_id  5\n",
      "row_bbox_list (inserting) after last word ['word_1_42', '50.00', [676, 272, 708, 282]]\n",
      "---------------------------line 6-----------------------------\n",
      "[['word_1_44', 'Context', [83, 288, 128, 298]], ['word_1_45', 'menu', [133, 291, 164, 298]], ['word_1_46', 'for', [169, 288, 184, 298]], ['word_1_47', 'invoices', [188, 288, 235, 298]], ['word_1_48', 'list', [239, 288, 255, 298]]]\n",
      "total word =  5\n",
      "curr_bbox :  [83, 288, 128, 298]\n",
      "next_bbox :  ['word_1_45', 'menu', [133, 291, 164, 298]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [83, 288, 164, 298]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_46', 'for', [169, 288, 184, 298]]\n",
      "dist  5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_bbox (merging) [83, 288, 184, 298]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_47', 'invoices', [188, 288, 235, 298]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [83, 288, 235, 298]\n",
      "next_bbox_id  4\n",
      "next_bbox :  ['word_1_48', 'list', [239, 288, 255, 298]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [83, 288, 255, 298]\n",
      "next_bbox_id  5\n",
      "row_bbox_list (inserting) after merge [83, 288, 255, 298]\n",
      "---------------------------line 7-----------------------------\n",
      "[['word_1_49', '2', [48, 318, 55, 328]], ['word_1_50', 'Develop', [76, 318, 123, 331]], ['word_1_51', 'x17.0', [412, 318, 443, 328]], ['word_1_52', 'hours', [464, 318, 496, 328]], ['word_1_53', '40.00', [587, 318, 619, 328]], ['word_1_54', '680.00', [669, 318, 708, 328]]]\n",
      "total word =  6\n",
      "curr_bbox :  [48, 318, 55, 328]\n",
      "next_bbox :  ['word_1_50', 'Develop', [76, 318, 123, 331]]\n",
      "dist  21.0\n",
      "row_bbox_list (inserting) [48, 318, 55, 328]\n",
      "curr_bbox (adding) [76, 318, 123, 331]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_51', 'x17.0', [412, 318, 443, 328]]\n",
      "dist  289.0\n",
      "row_bbox_list (inserting) [76, 318, 123, 331]\n",
      "curr_bbox (adding) [412, 318, 443, 328]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_52', 'hours', [464, 318, 496, 328]]\n",
      "dist  21.0\n",
      "row_bbox_list (inserting) [412, 318, 443, 328]\n",
      "curr_bbox (adding) [464, 318, 496, 328]\n",
      "next_bbox_id  4\n",
      "next_bbox :  ['word_1_53', '40.00', [587, 318, 619, 328]]\n",
      "dist  91.0\n",
      "row_bbox_list (inserting) [464, 318, 496, 328]\n",
      "curr_bbox (adding) [587, 318, 619, 328]\n",
      "next_bbox_id  5\n",
      "next_bbox :  ['word_1_54', '680.00', [669, 318, 708, 328]]\n",
      "dist  50.0\n",
      "row_bbox_list (inserting) [587, 318, 619, 328]\n",
      "curr_bbox (adding) [669, 318, 708, 328]\n",
      "next_bbox_id  6\n",
      "row_bbox_list (inserting) after last word ['word_1_54', '680.00', [669, 318, 708, 328]]\n",
      "---------------------------line 8-----------------------------\n",
      "[['word_1_56', 'Invoice', [84, 333, 124, 343]], ['word_1_57', 'number', [129, 333, 173, 343]], ['word_1_58', 'format', [177, 333, 214, 343]], ['word_1_59', 'template', [217, 333, 268, 346]]]\n",
      "total word =  4\n",
      "curr_bbox :  [84, 333, 124, 343]\n",
      "next_bbox :  ['word_1_57', 'number', [129, 333, 173, 343]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [84, 333, 173, 343]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_58', 'format', [177, 333, 214, 343]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [84, 333, 214, 343]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_59', 'template', [217, 333, 268, 346]]\n",
      "dist  3.0\n",
      "curr_bbox (merging) [84, 333, 268, 346]\n",
      "next_bbox_id  4\n",
      "row_bbox_list (inserting) after merge [84, 333, 268, 346]\n",
      "---------------------------line 9-----------------------------\n",
      "[['word_1_61', '[PRO]', [84, 349, 118, 362]], ['word_1_62', 'Duplicating', [124, 349, 187, 362]], ['word_1_63', 'invoices', [192, 352, 238, 359]]]\n",
      "total word =  3\n",
      "curr_bbox :  [84, 349, 118, 362]\n",
      "next_bbox :  ['word_1_62', 'Duplicating', [124, 349, 187, 362]]\n",
      "dist  6.0\n",
      "curr_bbox (merging) [84, 349, 187, 362]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_63', 'invoices', [192, 352, 238, 359]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [84, 349, 238, 362]\n",
      "next_bbox_id  3\n",
      "row_bbox_list (inserting) after merge [84, 349, 238, 362]\n",
      "---------------------------line 10-----------------------------\n",
      "[['word_1_65', 'Language', [84, 365, 141, 378]], ['word_1_66', 'support', [146, 366, 189, 378]]]\n",
      "total word =  2\n",
      "curr_bbox :  [84, 365, 141, 378]\n",
      "next_bbox :  ['word_1_66', 'support', [146, 366, 189, 378]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [84, 365, 189, 378]\n",
      "next_bbox_id  2\n",
      "row_bbox_list (inserting) after merge [84, 365, 189, 378]\n",
      "---------------------------line 11-----------------------------\n",
      "[['word_1_68', 'Context', [83, 380, 128, 390]], ['word_1_69', 'menu', [133, 383, 164, 390]], ['word_1_70', 'for', [169, 380, 184, 390]], ['word_1_71', 'invoices', [188, 380, 235, 390]], ['word_1_72', 'list', [239, 380, 255, 390]]]\n",
      "total word =  5\n",
      "curr_bbox :  [83, 380, 128, 390]\n",
      "next_bbox :  ['word_1_69', 'menu', [133, 383, 164, 390]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [83, 380, 164, 390]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_70', 'for', [169, 380, 184, 390]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [83, 380, 184, 390]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_71', 'invoices', [188, 380, 235, 390]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [83, 380, 235, 390]\n",
      "next_bbox_id  4\n",
      "next_bbox :  ['word_1_72', 'list', [239, 380, 255, 390]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [83, 380, 255, 390]\n",
      "next_bbox_id  5\n",
      "row_bbox_list (inserting) after merge [83, 380, 255, 390]\n",
      "---------------------------line 12-----------------------------\n",
      "[['word_1_73', '3', [48, 410, 55, 420]], ['word_1_74', 'Analysi', [75, 410, 114, 423]], ['word_1_75', '3.0', [415, 410, 439, 420]], ['word_1_76', 'hours', [464, 410, 496, 420]], ['word_1_77', '35.00', [587, 410, 619, 420]], ['word_1_78', '105.00', [670, 410, 708, 420]]]\n",
      "total word =  6\n",
      "curr_bbox :  [48, 410, 55, 420]\n",
      "next_bbox :  ['word_1_74', 'Analysi', [75, 410, 114, 423]]\n",
      "dist  20.0\n",
      "row_bbox_list (inserting) [48, 410, 55, 420]\n",
      "curr_bbox (adding) [75, 410, 114, 423]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_75', '3.0', [415, 410, 439, 420]]\n",
      "dist  301.0\n",
      "row_bbox_list (inserting) [75, 410, 114, 423]\n",
      "curr_bbox (adding) [415, 410, 439, 420]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_76', 'hours', [464, 410, 496, 420]]\n",
      "dist  25.0\n",
      "row_bbox_list (inserting) [415, 410, 439, 420]\n",
      "curr_bbox (adding) [464, 410, 496, 420]\n",
      "next_bbox_id  4\n",
      "next_bbox :  ['word_1_77', '35.00', [587, 410, 619, 420]]\n",
      "dist  91.0\n",
      "row_bbox_list (inserting) [464, 410, 496, 420]\n",
      "curr_bbox (adding) [587, 410, 619, 420]\n",
      "next_bbox_id  5\n",
      "next_bbox :  ['word_1_78', '105.00', [670, 410, 708, 420]]\n",
      "dist  51.0\n",
      "row_bbox_list (inserting) [587, 410, 619, 420]\n",
      "curr_bbox (adding) [670, 410, 708, 420]\n",
      "next_bbox_id  6\n",
      "row_bbox_list (inserting) after last word ['word_1_78', '105.00', [670, 410, 708, 420]]\n",
      "---------------------------line 13-----------------------------\n",
      "[['word_1_80', '[PRO]', [84, 426, 118, 439]], ['word_1_81', 'Duplicating', [124, 426, 187, 439]], ['word_1_82', 'invoices', [192, 429, 238, 436]]]\n",
      "total word =  3\n",
      "curr_bbox :  [84, 426, 118, 439]\n",
      "next_bbox :  ['word_1_81', 'Duplicating', [124, 426, 187, 439]]\n",
      "dist  6.0\n",
      "curr_bbox (merging) [84, 426, 187, 439]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_82', 'invoices', [192, 429, 238, 436]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [84, 426, 238, 439]\n",
      "next_bbox_id  3\n",
      "row_bbox_list (inserting) after merge [84, 426, 238, 439]\n",
      "---------------------------line 14-----------------------------\n",
      "[['word_1_84', 'Language', [84, 442, 141, 455]], ['word_1_85', 'support', [146, 443, 189, 455]]]\n",
      "total word =  2\n",
      "curr_bbox :  [84, 442, 141, 455]\n",
      "next_bbox :  ['word_1_85', 'support', [146, 443, 189, 455]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [84, 442, 189, 455]\n",
      "next_bbox_id  2\n",
      "row_bbox_list (inserting) after merge [84, 442, 189, 455]\n",
      "---------------------------line 15-----------------------------\n",
      "[['word_1_90', '835.00', [665, 488, 709, 499]]]\n",
      "total word =  1\n",
      "curr_bbox :  [665, 488, 709, 499]\n",
      "row_bbox_list (inserting) after merge [665, 488, 709, 499]\n",
      "---------------------------line 16-----------------------------\n",
      "[['word_1_91', 'Tax', [411, 513, 436, 524]], ['word_1_92', '(18.0%):', [440, 513, 494, 527]], ['word_1_93', '150.30', [666, 513, 709, 524]]]\n",
      "total word =  3\n",
      "curr_bbox :  [411, 513, 436, 524]\n",
      "next_bbox :  ['word_1_92', '(18.0%):', [440, 513, 494, 527]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [411, 513, 494, 527]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_93', '150.30', [666, 513, 709, 524]]\n",
      "dist  172.0\n",
      "row_bbox_list (inserting) [411, 513, 494, 527]\n",
      "curr_bbox (adding) [666, 513, 709, 524]\n",
      "next_bbox_id  3\n",
      "row_bbox_list (inserting) after last word ['word_1_93', '150.30', [666, 513, 709, 524]]\n",
      "---------------------------line 17-----------------------------\n",
      "[['word_1_94', 'Discount', [412, 537, 472, 548]], ['word_1_95', '(10.0%):', [477, 537, 531, 551]], ['word_1_96', '-83.50', [668, 537, 709, 548]]]\n",
      "total word =  3\n",
      "curr_bbox :  [412, 537, 472, 548]\n",
      "next_bbox :  ['word_1_95', '(10.0%):', [477, 537, 531, 551]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [412, 537, 531, 551]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_96', '-83.50', [668, 537, 709, 548]]\n",
      "dist  137.0\n",
      "row_bbox_list (inserting) [412, 537, 531, 551]\n",
      "curr_bbox (adding) [668, 537, 709, 548]\n",
      "next_bbox_id  3\n",
      "row_bbox_list (inserting) after last word ['word_1_96', '-83.50', [668, 537, 709, 548]]\n",
      "---------------------------line 18-----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['word_1_97', 'Total', [410, 562, 445, 573]], ['word_1_98', '(EUR):', [450, 562, 494, 576]], ['word_1_99', '901.80', [665, 562, 709, 573]]]\n",
      "total word =  3\n",
      "curr_bbox :  [410, 562, 445, 573]\n",
      "next_bbox :  ['word_1_98', '(EUR):', [450, 562, 494, 576]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [410, 562, 494, 576]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_99', '901.80', [665, 562, 709, 573]]\n",
      "dist  171.0\n",
      "row_bbox_list (inserting) [410, 562, 494, 576]\n",
      "curr_bbox (adding) [665, 562, 709, 573]\n",
      "next_bbox_id  3\n",
      "row_bbox_list (inserting) after last word ['word_1_99', '901.80', [665, 562, 709, 573]]\n",
      "---------------------------line 19-----------------------------\n",
      "[['word_1_100', 'Your', [42, 612, 69, 622]], ['word_1_101', 'billing', [74, 612, 106, 625]], ['word_1_102', 'information', [110, 612, 174, 622]], ['word_1_103', '(Bank,', [179, 612, 216, 625]], ['word_1_104', 'Address,', [220, 612, 272, 624]], ['word_1_105', 'IBAN,', [277, 612, 310, 624]], ['word_1_106', 'SWIFT', [314, 612, 355, 622]], ['word_1_107', '&', [359, 612, 367, 622]], ['word_1_108', 'etc.)', [371, 612, 396, 625]]]\n",
      "total word =  9\n",
      "curr_bbox :  [42, 612, 69, 622]\n",
      "next_bbox :  ['word_1_101', 'billing', [74, 612, 106, 625]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [42, 612, 106, 625]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_102', 'information', [110, 612, 174, 622]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [42, 612, 174, 625]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_103', '(Bank,', [179, 612, 216, 625]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [42, 612, 216, 625]\n",
      "next_bbox_id  4\n",
      "next_bbox :  ['word_1_104', 'Address,', [220, 612, 272, 624]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [42, 612, 272, 625]\n",
      "next_bbox_id  5\n",
      "next_bbox :  ['word_1_105', 'IBAN,', [277, 612, 310, 624]]\n",
      "dist  5.0\n",
      "curr_bbox (merging) [42, 612, 310, 625]\n",
      "next_bbox_id  6\n",
      "next_bbox :  ['word_1_106', 'SWIFT', [314, 612, 355, 622]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [42, 612, 355, 625]\n",
      "next_bbox_id  7\n",
      "next_bbox :  ['word_1_107', '&', [359, 612, 367, 622]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [42, 612, 367, 625]\n",
      "next_bbox_id  8\n",
      "next_bbox :  ['word_1_108', 'etc.)', [371, 612, 396, 625]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [42, 612, 396, 625]\n",
      "next_bbox_id  9\n",
      "row_bbox_list (inserting) after merge [42, 612, 396, 625]\n",
      "---------------------------line 20-----------------------------\n",
      "[['word_1_109', 'Was', [42, 641, 68, 651]], ['word_1_112', 'Beschreibung', [110, 641, 189, 654]], ['word_1_110', 'ist', [72, 642, 85, 651]], ['word_1_111', 'mit', [89, 642, 106, 651]]]\n",
      "total word =  4\n",
      "curr_bbox :  [42, 641, 68, 651]\n",
      "next_bbox :  ['word_1_112', 'Beschreibung', [110, 641, 189, 654]]\n",
      "dist  42.0\n",
      "row_bbox_list (inserting) [42, 641, 68, 651]\n",
      "curr_bbox (adding) [110, 641, 189, 654]\n",
      "next_bbox_id  2\n",
      "next_bbox :  ['word_1_110', 'ist', [72, 642, 85, 651]]\n",
      "dist  117.0\n",
      "row_bbox_list (inserting) [110, 641, 189, 654]\n",
      "curr_bbox (adding) [72, 642, 85, 651]\n",
      "next_bbox_id  3\n",
      "next_bbox :  ['word_1_111', 'mit', [89, 642, 106, 651]]\n",
      "dist  4.0\n",
      "curr_bbox (merging) [72, 642, 106, 651]\n",
      "next_bbox_id  4\n",
      "row_bbox_list (inserting) after merge [72, 642, 106, 651]\n",
      "---------------------------line 21-----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a,b = row_merge(lines,30)\n",
    "a,b,c = row_clustring(lines,10)\n",
    "# idx=1\n",
    "display_bounding_box(img, word_bbox_list,'orginal words')\n",
    "img=cv2.imread(file_name[idx])\n",
    "display_bounding_box(img, a, 'after row clusting' )\n",
    "# d= col_clestring(c,10)\n",
    "# img=cv2.imread(file_name[idx])\n",
    "# display_bounding_box(img, d,'after col clusting' )\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['word_1_4', '\"Romashka\"', [49, 44, 145, 54]],\n",
       "   ['word_1_5', 'Ltd.', [125, 44, 145, 54]]],\n",
       "  [['word_1_1', 'Invoice', [411, 34, 483, 45]],\n",
       "   ['word_1_2', 'ID:', [465, 34, 483, 45]]]],\n",
       " [[['word_1_6', '1600', [49, 60, 368, 73]],\n",
       "   ['word_1_7', 'Amphitheatre', [81, 60, 159, 73]],\n",
       "   ['word_1_8', 'Parkway', [164, 60, 213, 73]],\n",
       "   ['word_1_9', 'Mountain', [217, 60, 270, 70]],\n",
       "   ['word_1_10', 'View,', [275, 60, 306, 72]],\n",
       "   ['word_1_11', 'CA', [311, 60, 328, 70]],\n",
       "   ['word_1_12', '94043', [332, 60, 368, 70]]],\n",
       "  [['word_1_13', 'Invoice', [411, 59, 498, 70]],\n",
       "   ['word_1_14', 'date:', [465, 59, 498, 70]]]],\n",
       " [[['word_1_16', 'Due', [411, 83, 476, 94]],\n",
       "   ['word_1_17', 'date:', [442, 83, 476, 94]]]],\n",
       " [[['word_1_19', 'Text', [411, 108, 520, 119]],\n",
       "   ['word_1_20', 'custom', [444, 109, 495, 119]],\n",
       "   ['word_1_21', 'fiel', [500, 108, 520, 119]]]],\n",
       " [[],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [['word_1_32', 'Unit', [520, 234, 620, 263]],\n",
       "   ['word_1_33', 'price', [550, 234, 583, 263]],\n",
       "   ['word_1_34', '(EUR)', [587, 243, 620, 256]]]],\n",
       " [[], [], [], []],\n",
       " [],\n",
       " [[], [], [], [], []],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[], [], [], [], []],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[['word_1_91', 'Tax', [411, 513, 494, 527]],\n",
       "   ['word_1_92', '(18.0%):', [440, 513, 494, 527]]]],\n",
       " [[['word_1_94', 'Discount', [412, 537, 531, 551]],\n",
       "   ['word_1_95', '(10.0%):', [477, 537, 531, 551]]]],\n",
       " [[['word_1_97', 'Total', [410, 562, 494, 576]],\n",
       "   ['word_1_98', '(EUR):', [450, 562, 494, 576]]]],\n",
       " [],\n",
       " [[], []]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[49, 44, 145, 54], [411, 34, 483, 45], [567, 34, 679, 45]],\n",
       " [[], [], [], [], []])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0],b[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_bounding_box(img, word_bbox_list,title=\"word bounding box\"):\n",
    "    for bbox in word_bbox_list: \n",
    "        if len(bbox)==3:\n",
    "            b = bbox[-1]\n",
    "#             print(bbox)\n",
    "        else:\n",
    "            b=bbox\n",
    "        img = cv2.rectangle(img, (int(b[0]), int(b[1])), (int(b[2]), int(b[3])), (0, 255, 0), 2)\n",
    "\n",
    "    # show annotated image and wait for keypress\n",
    "    cv2.namedWindow(title, cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(title, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=\"C:/Users/Sanjeev/Downloads/software/0325updated.task2train(626p)-20200414T125907Z-001/0325updated.task2train(626p)/X00016469612.jpg\"\n",
    "def print_ocr_string(img):  \n",
    "    res = isinstance(img, str)\n",
    "    if res:\n",
    "        print('reading image ../n' )\n",
    "        img = cv2.imread(img)  \n",
    "    text = pytesseract.image_to_string(img)\n",
    "#     print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ocr_string(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bpemb"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
